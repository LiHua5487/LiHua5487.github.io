
协方差
$$
Cov(X,Y) = E(XY)-E(X)E(Y)
$$

相关系数
$$
\rho_{XY} = \frac{Cov(X,Y)}{\sqrt D(X)\sqrt D(Y)}
$$

# 基本信息

- 智能体：通过传感器感知 percept 环境并通过执行器 actuator/effectors 作用于该环境的事物都可以被视为智能体

搜索四要素：状态空间、后继函数、开始状态、目标检测
基本组成包括
1. 目标 goal
	即我们要去“找”什么；什 么时候可以结束搜索 
2. 状态 state
	主要包括三部分，开始状态 initial states ，目标状态 goal states ，当前状态 current state  
3. 动作 actions
	智能体可以采取的行动/ 决策 
4. （状态）转移方程 transition function
	当前状态随着动作会怎么变化 
5. 代价函数 cost function
	每个动作要消 耗多大的成本

解决步骤
1. 构建一个列表，表示现在可以考虑的状态
2. 如果列表为空，表示失败了
3. 如果列表里包含某个目标状态，表示成功了
4. 从列表弹出一个节点，代表当前节点
	注意移头还是移尾？
5. 把某些和当前节点相关的节点，加入列表
	加某一个还是都加入？

对于不同的搜索算法，关键在于优先弹出什么节点
要在取出节点时判断是不是目标，而不是在放入节点时判断

# 搜索算法

## BFS

1. 首先将根节点放入列表中 
2. 从列表中取出第一个节点，并检验
	检测是否为目标，如果找到目标，则结束搜寻并回传结果
	之后检测这个节点有没有访问过
	如果没有，将它所有尚未检验过的直接子节点加入队列中 
3. 若列表为空，表示整张图都检查过了，即图中没有欲搜寻的目标
	结束搜寻并回传“找不到目标”
4. 重复执行以上过程

BFS 与 DFS：先入先出 queue / 后入先出 stack

深搜不能保证路径最短，但广搜可以
如果考虑路径的代价，都不能保证最优（代价最小）

## UCS 一致代价搜索

沿着一条路径，从根节点到当前节点的代价总和记为 g
优先弹出 g 最小的节点 priority queue
如果当前节点已经访问过，说明已经有总代价更小的路径了，就不用访问

![[AI引论/AIimg/img0/image.png|592x478]]

- 完备性
	假设最优路径的代价有限，并且边的最低代价是正的，那 UCS 就是完备的
	即如果存在某种能到达目标的解法，该算法一定能找到
- 最优性
	可以证明，UCS 得到的结果一定最优 optimal ，即总代价最小

但是 UCS 并没有“全局视野”，没有考虑当前节点相对于目标的信息

priority queue
- 优先值低的先出 
- 通常依靠堆 heap 来实现

![[AI引论/AIimg/img0/image-1.png]]

核心目的
- 维持一定的顺序，值最小的在堆根
特性
- 依靠树的层级来维持顺序，父节点一定小于等于子节点 
- 树是完整的，即只有最底层的右边有空缺

最小堆
- 根节点优先级总是小于叶节点
- 是一颗“完全树”，即除了最底层， 其他层的节点都被元素填满，且最底层尽可能地从左到右填入

以下两种都不是最小堆，分别违反了
- 根节点优先级 < 叶节点 (20 > 15)
- 最底层要从左到右填入 (700 在右边)

![[AI引论/AIimg/img0/image-2.png|462x250]]

最小堆通过以下方法维持

当一个节点加进来时，先加到最底层的最右边
然后与父节点比较优先级大小，逐渐往上移

![[AI引论/AIimg/img0/image-3.png|505x209]]

弹出一个节点后（根节点，其优先级最小），把最底层最右边的节点放到根节点

![[AI引论/AIimg/img0/image-4.png|316x224]]

与下面的子节点比较优先级大小，往小的那边移动，都更大或者到底了就停下

![[AI引论/AIimg/img0/image-5.png|591x226]]

## 启发搜索

那有没有能够利用目标信息的搜索？

盲目搜索： DFS BFS UCS
知情搜索：启发 贪心 A*

**启发 heuristic** ：估计当前状态离目标状态还差多少，记为 h 
- 目标点的启发函数固定为 0
- 一般可以用距离信息估计
	比如对于数字华容道，可以用每个数字到正确位置的曼哈顿距离和估计

### 贪心

优先选择 h 最小的节点
不一定最优，没考虑走到待扩展节点的代价，而且启发估计不一定准确

### A*

- UCS 向后看，根据至今积累的代价排序 g(n) 
- 贪心向前看, 根据离目标还有多远的估计排序 h(n)
- A* 根据两者的和决定顺序 f(n) = g(n) + h(n)

即，优先弹出 f(n) 最小的节点

但是 A* 也依赖于启发函数，所以结果不一定最优
比如一条路径明明是最优的，但是启发函数估值很大，就不会先走这条路
也就是说，要避免估计值太大了而破坏最优性

那什么时候能保证结果一定最优呢？

**可接受的启发**
当启发函数满足
$$0\leqslant h(n)\leqslant h^*(n)$$
- $n$ 代表当前节点的信息
- $h(n)$ 为启发函数，$h^*(n)$ 为当前到目标的最小的真实代价

可以证明，当启发函数可接受时，结果一定最优

![[AI引论/AIimg/img0/image-6.png|252x185]]

假设 $A$ 是一个最优的目标节点，$B$ 是一个次优的目标节点，即满足 
$$g(A) < g(B)$$
$h$ 是可接受的启发函数，满足
$$0 \leq h(n) \leq h^*(n) = g(A) - g(n)$$
我们需要证明：$A$ 一定比 $B$ 更早离开优先队列

假设 B 已经在队列里，A 的一些前序节点 n 也在队列里（A 也是自己的前序）
那么需要证明：n 比 B 先离开队列，也就是要证
$$f(n)\leqslant f(B)$$
对于 n ，有
$$f(n)=g(n)+h(n)\leqslant g(n)+h^*(n)=g(A)$$
由于 A 是目标节点，$h(A)=0$ ，有 $g(A)=f(A)$，故
$$f(n)\leqslant f(A)$$

另一方面，由于 B 也是目标节点，有 $g(B)=f(B)$
由于 B 是次优的，有 $g(A)<g(B)$ ，则 
$$f(A)<f(B)$$
综合以上可得
$$f(n)\leqslant f(A) < f(B)$$
也就是说明 n 比 B 先离开队列，即 A 的前序节点永远比 B 先离开
那么最终 A 就会比 B 先离开，找到的结果就是 A ，也就说明一定是最优的

那能不能直接用真实代价作为启发？
当启发越来越接近真实代价的时候，我们可能可以扩展更少的节点，但很有可能我们需要在每一个节点做更多的计算从而达到这个启发
获取 $h^*(n)$ 的成本极高，而且通常无法直接获得

在去重时，根据 g 来判断，如果当前节点已经访问过，且此时的 g 大于之前访问时的 g ，那就说明有更优的路径到达当前节点，就不用再考虑此时的路径了

更严格的要求：**一致性**

可接受要求当前到目标的估计的总代价小于等于实际总代价
而一致性要求每一条边的估计代价都要小于这条边的实际代价
考虑经过一条边从 A 到 C ，由于 h 是相对于目标点而言，那么估计的这条边的代价就是 $h(A)-h(C)$ ，需要满足
$$h(A)\ –\ h(C) ≤ cost(A \to C)$$
这保证了沿着搜索路径，f 永远不会变小
$$
f(A)=g(A)+h(A)\leqslant g(A)+cost(A\to C)+h(C)=g(C)+h(C)=f(C)
$$
满足可接受或者一致性都能保证结果一定最优
当启发函数仅满足可接受性，可能会访问同一个节点多次
然而，如果启发函数是一致的，那不再访问去过的节点，也能保证最优性

# 对抗搜索

零和博弈 Zero-Sum Games 
- 效用：关于回报大小的度量 
- 智能体的效用是对立的
	当一个智能体最大化其效用时，另一智能体效用为其最小值 
	所有智能体效益总和为 0
- 特点：对抗的，完全竞争

一般博弈 General Games)
- 智能体获得的效用是独立的 
- 智能体之间可能的关系：合作、无关、竞争... 
- 我们希望AI不是孤立地行动，而是与人类共事或是协助人类，因此每个AI智能体都需要面对和处理博弈的情况

问题建模
- 状态(States): 𝑆 (起始状态为 𝑠0) 
- 博弈主体(Players): 𝑃 = {1…𝑁} (通常轮流进行) 
- 行动(Actions): 𝐴 (可能取决于 玩家/ 状态) 
- 转移方程(Transition Function): 𝑆 × 𝐴 → 𝑆 
- 结束测试(Terminal Test): 𝑆 → {𝑡,𝑓} 
- 最终效用(Terminal Utilities): 𝑆 × 𝑃 → 𝑅 

应对方案称为策略(policy): 𝑆 → A

在对抗搜索中，不再最小化代价，而是最大化智能体所得的分数/效用
状态价值: 从该状态出发可能获得的最大最终效用

## minimax

状态空间的搜索树
- 玩家轮流行动，max 层代表己方，min 层代表对手
- 计算每个节点极大极小价值
	假设对手是理性的（即对方总是努力达到最优），自己能达到的最大效用

在考虑下一步行动时，会先预测后续的状态

![[AI引论/AIimg/img0/image-7.png]]

对于这样一个搜索树，达到最底层时（由搜索深度确定），使用评估函数估计当前状态的价值
而后从下往上返回，对于 min 层，对手要最大化自己的利益，如果只有双方，那就是要最小化己方的利益，选取子节点的最小值
对于 max 层相反，选取子节点的最大值
最终根节点代表当前状态下能保证的最大收益
从下一层中选择收益最大的一步行动

为了防止搜索树过大，可以用 **alpha-beta 剪枝**

- alpha ：上方的max层目前至少能取多少，初始化为 $-\infty$
- beta ：上方的min层目前至多能取多少，初始化为 $\infty$

![[AI引论/AIimg/img0/image-8.png|489x277]]

比如对于这个搜索树，从左往右考虑，min 层的 A 节点得到 8
这代表上面的 max 层至少能取到 8 ，即 A 所在层的 $\alpha = 8$
而后考虑右边的 B ，从子节点 4 可以得到，上面的 min 至多为 4 ，
而 $4<8=\alpha$ ，即 max 已经确定至少是 8 ，就不用再考虑这边了
那剩下的没被访问的 50 就相当于被剪掉了

min 层使用 alpha 剪枝，max 层使用 beta 剪枝
这种剪枝不影响根节点的值，但中间节点的值不一定是对的

![[AI引论/AIimg/img0/image-9.png]]

估值函数为非终止状态打分
理想的估值函数：返回当前局面的实际极大极小值
实际的估值函数：通常是特征的加权线性和
可以用机器学习获得

对于非零和博弈，或者有多方参与，将 minimax 推广
- 终止状态会给出效用元组 
- 节点的价值也是效用元组 
- 每个玩家最大化自己的部分 
这样可以动态地产生合作和竞争

![[AI引论/AIimg/img0/image-10.png|535x308]]

## 期望最大化搜索

对手行动可能存在偶然因素，因此，要让期望最大化而不是单纯的考虑最坏情况
这时，需要把 minimax 中的 min 层换成 chance 层，计算子节点的期望

## MCTS 蒙特卡洛搜索

考虑一个玩老虎机的场景，不同老虎机的收益期望不同
每次选择一个，想让收益最大化，也就是说要尽可能找出收益期望最大的老虎机

![[AI引论/AIimg/img0/image-11.png]]

可以得到对下一时间的动作的估值
$$
Q_{t+1}(a) = 
\begin{cases} 
\frac{1}{N_t(a)} \sum_{\{\tau | \tau \leq t, A_\tau = a\}} R_\tau, & N_t(a) > 0 \\ 
0, & N_t(a) = 0
\end{cases}
$$

- $k$: 动作的数量
- $q_*(a)$: 动作 $a$ 的真实价值
- $Q_t(a)$: 在时间 $t$ 的时候，对动作 $a$ 的估值
- $N_t(a)$: 在时间 $t$ 及之前，动作 $a$ 被选中的次数
- $R_t$: 在时间 $t$ 得到的价值
- $A_t$: 在时间 $t$ 采取的动作

对于当前状况，有两种策略
- 利用 exploitation ：选取当前期望最大的
- 探索 exploration ：选取当前期望不是最大的
矛盾在于，既想抓住最优的玩下去，又怕自己判断错

采用 **ε-greedy 方法**
- $1-\epsilon$ 的概率：选当前最优的 
- $\epsilon$ 的概率：随机选取一个当前不是最优的

探索是永远需要的，因为我们的估计永 远有不确定性
利用只是针对当前看起来最优的，别的动作有可能更好
ε-greedy 强制非最优动作也会被尝试， 但对所有非最优动作一视同仁
如果我们可以更多地去尝试那些更有希 望成为更优的动作，算法的效率将更高

### 最大置信 Upper Confidence Bound

尝试次数越多估值越准确，也就是说我们对尝试次数少的估计信任不足
这时候就需要更多的探索
算法应平衡估值的大小和次数的多少
$$
A_{t+1} = \arg\max_a \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$
-  $A_{t+1}$ 表示在 $t+1$ 时刻选择的动作
-  $\arg\max$ 意思是选择动作 $a$，使得括号内的表达式达到最大值
-  $Q_t(a)$ 表示当前对动作 $a$ 的估计值，对应利用
-  $c$ 是超参数，用于调整探索项，影响探索和利用的平衡
-  $\sqrt{\frac{\ln t}{N_t(a)}}$ 是探索项
	- $N_t(a)$：表示截止到第 $t$ 步时动作 $a$ 被选择的次数
	- $\ln t$：随着时间 $t$ 的增加，探索强度的增长逐渐减弱

结合 UCB，MCTS 有以下四个主要过程

![[AI引论/AIimg/img0/image-12.png]]

1. 选择 selection ：选取各个子节点中对于父节点 UCB 最大的一个 

以下棋为例，之前已经考虑过一些情况，构建搜索树

![[AI引论/AIimg/img0/image-13.png]]

当前处在顶上的根节点，也就是说总共经过 6 步，即 t = 6 
对于下面左边的子节点，走了 3 次，即 N = 3
其中有 2 次获胜，那么 Q = 2/3 ，取 c = 0.5 ，计算得到 UCB = 1.05
另外的子节点同理，可以得到左侧的子节点 UCB 最大，就选这个走法
如果父节点没有完全扩展，那就优先进行扩展（相当于 0/0 节点 UCB 无限大）

![[AI引论/AIimg/img0/image-14.png|323x293]]

现在到了左侧的黑棋节点 (2/3) ，以这个节点为根节点，重复上面的过程
此时做出的选择代表对手认为的最优选择
直到最后到达了最底下的叶子节点

2. 扩展 expansion ：对于没有完全扩展的父节点，或者到了最底下，之后的走法情况没记录过，要进行扩展

扩展方式
- 单子节点扩展：每次扩展一个新的子节点（常用） 
- 全子节点扩展：一次性扩展所有可能的子节点

现在节点 (1/1) 下没有更多的统计记录，进行一次随机移动并扩展一个新的节点

![[AI引论/AIimg/img0/image-15.png|390x384]]

3. 模拟 roll-out / simulation ：对于扩展节点，随机选择走法，模拟一局游戏

在实际使用 MCTS 的时候，模拟经常不会选择一直运行到博弈结束，而是会设置一个最大步数， 然后直接返回启发值

4. 回溯 backpropagation：模拟后，根据输赢结果更新所选路径上的节点的获胜次数与经过次数

![[AI引论/AIimg/img0/image-16.png]]

重复上面的过程 X 次，而后选择 UCB 最大的一步走

