---
date: 2025-04-17
tags:
  - AI
---
# 基本内容
## Context-Free Grammar (CFG)

- 句法 syntax ：规定如何用单词构成句子
	- 句法下的句子可能有歧义
- 语义 semantic ：语言的含义
	- 词汇含义、组合含义
	- 不同句子可能有相同含义
	- 一些符合句法的句子可能狗屁不通
- 文法 grammar ：一组规则，定义了合法短语的结构 
- 语言 language ：遵循文法的句子集

问题在于，自然语言无法像形式语言一样清晰的表达
- 可能有不同解读
- 可能有歧义，本身意思就模糊不清
- 自然语言没有正式定义从符号到对象的映射

- 非终结符：用来表示语法规则中的中间组成部分
	- 可以进一步分解为其他非终结符或终结符
- 终结符：代表具体的单词、字符或语言单元
	- 直接出现在输入串中，无法进一步分解

非终结符通过语法规则逐步展开，最终生成由终结符组成的句子

以下面定义的语法规则为例

- 非终结符 $\to$ 终结符
	名词 N → she | city | car | Harry | ... 
	冠词 D → the | a | an | ... 
	动词 V → saw | ate | walked | ... 
	介词 P → to | on | over | ... 
	形容词 ADJ → blue | busy | old | ...

其中 | 代表”或“

下面定义 非终结符 $\to$ 非终结符

![[AI引论/AIimg/img4/image.png]]

其中 形如 $A \to BC\ |\ D$ 表示 A 可以拆分成 B 和 C，或者直接由 D 组成
注意，这里的 BC 是有先后顺序的
S 一般是一个句子的起始，对应生成树的根节点

- 上下文无关：任一规则都可以在任何上下文使用
- 概率上下文无关的文法：每个对应规则对应多种情况，有不同概率

![[AI引论/AIimg/img4/image-1.png]]

## Parsing

句法分析 Parsing ：根据文法规则分析一串单词以获得其短语结构的过程

一种方法是，自顶向下，从 S 逐渐展开
但问题在于，这种方式很低效，而且容易陷入死循环

另一种方法，自底向上，从终结符开始逐渐还原上一层
但是也容易陷入盲目搜索，可以利用动态规划的想法

![[AI引论/AIimg/img4/image-2.png]]

### CYK 算法

- CNF 乔姆斯基范式：语法中的生成规则符合以下形式
	- $A \to BC$ ：非终结符二分叉
	- $A \to a$ ：非终结符到终结符
	- $S \to \emptyset$ ：可以直接生成空串

据此，CNF 的生成树的所有节点都是二分叉的

任何语法都可以转化成一个弱等价的 CNF 形式，比如：
- $A \to B c$ 转化为 $A \to B C, C \to c$ 
- $A \to B C D$ 转化为 $A \to B X, X \to C D$ 

而 CYK 算法处理的 CFG 必须是 CNF 形式的

以下面这个语法为例
$$
\begin{align}
S &\to A\ B \ | \ B\ C \\
A &\to B\ A \ | \ a \\
B &\to C\ C \ | \ b \\
C &\to A\ B \ | \ a \\
\end{align}
$$
输入 $w\ =\ 'baaba'$ ，其中 a 和 b 代表终结符，想要求出其生成树

先画出这么一个表格
最底下一行代表直接生成每一个终结符的所有可能的非终结符的集合

|     | 1             | 2             | 3             | 4             | 5             |
| --- | ------------- | ------------- | ------------- | ------------- | ------------- |
| 5   | X(1,5)        | ------        | ------        | ------        | ------        |
| 4   | X(1,4)        | X(2,5)        | ------        | ------        | ------        |
| 3   | X(1,3)        | X(2,4)        | X(3,5)        | ------        | ------        |
| 2   | X(1,2)        | X(2,3)        | X(3,4)        | X(4,5)        | ------        |
| 1   | X(1,1)<br>'b' | X(2,2)<br>'a' | X(3,3)<br>'a' | X(4,4)<br>'b' | X(5,5)<br>'a' |

比如 $X(1,1)$ 就对应 w 中第一个终结符 b ，可以由 B 直接生成，即 $X(1,1) = B$
而 a 可由 A 或 C 直接生成，可以填出来最后一行

|     | 1          | 2            | 3            | 4          | 5            |
| --- | ---------- | ------------ | ------------ | ---------- | ------------ |
| 5   |            | ------       | ------       | ------     | ------       |
| 4   |            |              | ------       | ------     | ------       |
| 3   |            |              |              | ------     | ------       |
| 2   |            |              |              |            | ------       |
| 1   | {B}<br>'b' | {A,C}<br>'a' | {A,C}<br>'a' | {B}<br>'b' | {A,C}<br>'a' |

对于行号 2 ，即倒数第2行，对应连着的2个终结符，比如 $X(1,2)$ 对应 $'ba'$
要找到能生成这种字串的非终结符
因为 CNF 是二分叉的，考虑将这个字串进行二分
即 $'ba'$ 可看作 b 和 a 组成，而 b 和 a 分别由 B 和 A/C 直接生成
也就是要找到能分叉成 BA 或 BC 的非终结符，可以是 S 或 A
同理可填出行号 2

|     | 1             | 2            | 3             | 4             | 5            |
| --- | ------------- | ------------ | ------------- | ------------- | ------------ |
| 5   |               | ------       | ------        | ------        | ------       |
| 4   |               |              | ------        | ------        | ------       |
| 3   |               |              |               | ------        | ------       |
| 2   | {S,A}<br>'ba' | {B}<br>'aa'  | {S,C}<br>'ab' | {S,A}<br>'ba' | ------       |
| 1   | {B}<br>'b'    | {A,C}<br>'a' | {A,C}<br>'a'  | {B}<br>'b'    | {A,C}<br>'a' |

对于行号 3 ，对应连着的3个终结符，比如 $X(1,3)$ 对应 $'baa'$
将 $'baa'$ 二分，其可由 $'b'+'aa'$ 或 $'ba'+'a'$ 构成
对于前者，要能分叉成 BB ，对于后者，要能分叉成 SA/AA/SC/AC
但都找不到能这么分的非终结符，所以是空集
注意，这不代表 w 不能由这个语法生成，因为这只是针对 $'baa'$ 而言
这样行号 3 也能填出来

之后的行号同理，最后得到下表

|     | 1                       | 2                 | 3             | 4             | 5            |
| --- | ----------------------- | ----------------- | ------------- | ------------- | ------------ |
| 5   | {S,A,C}<br>'baaba'      | ------            | ------        | ------        | ------       |
| 4   | {$\emptyset$}<br>'baab' | {S,A,C}<br>'aaba' | ------        | ------        | ------       |
| 3   | {$\emptyset$}<br>'baa'  | {B}<br>'aab'      | {B}<br>'aba'  | ------        | ------       |
| 2   | {S,A}<br>'ba'           | {B}<br>'aa'       | {S,C}<br>'ab' | {S,A}<br>'ba' | ------       |
| 1   | {B}<br>'b'              | {A,C}<br>'a'      | {A,C}<br>'a'  | {B}<br>'b'    | {A,C}<br>'a' |

最终的格 $X(1,5)$ 包含 S ，说明 w 可以由这个语法生成
要构建生成树，从上往下按照对应关系二分叉即可

由于整个是一个树结构，也可以采用 A* 搜索

![[AI引论/AIimg/img4/image-3.png]]

## Token 

token ：句子中的最小基本单元
一个句子可以被分割成一组 token，比如句子`I love NLP.`
- 如果按单词分割，每个 "单词" 是一个 token：
	`["I", "love", "NLP", "."]`
- 如果按字符分割，每个 "字符" 是一个 token：
	`["I", " ", "l", "o", "v", "e", " ", "N", "L", "P", "."]`
- 如果是子词分割（如使用 BPE 算法），可能分成：
	`["I", "lo", "ve", "N", "LP", "."]`

### Tokenization 分词

- 单词切分：将句子分为独立的单词
	- 不能简单地按照空格分割，比如可能含有标点符合
	- 还可能含有其他符号，比如 ' ，连字符
- 句子切分：将一段话分为一堆句子
	- 不能简单地按照句号分割，比如可能有 Mr.

对于中文，情况更为复杂，因为词与词都是连一块的
因此，中文分词是很多 NLP 中的基础模块和首要环节
主要有以下方法
- 基于字符串匹配的分词方法 
- 基于机器学习的分词方法 
jieba 是一个常用的中文分词工具包

# 基于统计的 NLP 模型

## BoW 词袋模型

- 数据收集：一个语料库包含若干行文本，每行文本视为一个文档
- 构建词典：列出所有出现的词
- 生成文本特征向量：统计每个词在每个文档出现的概率
	- 特征向量每一位分别代表词汇表中的所有单词
	- 向量中的每一位的值表示该单词在输入文本中的出现次数

![[AI引论/AIimg/img4/image-4.png]]

缺点
- 句子顺序丢失
- 字典可能非常大
- 特征向量稀疏
- 关键词的重要性无法体现

不过即便是这样，对于一些比较简单的场合，也有比较好的效果
比如文本情感分类，判断是正面的还是负面的

## 朴素贝叶斯模型

贝叶斯公式
$$P(B_i|A) = \frac{P(B_i)P(A|B_i)}{\sum_{j=1}^n P(A|B_j)P(B_j)}$$

对于输入的一句话，要求出其情感为正负面的概率
基本假设：不考虑上下文关系，那一句话就可以拆成一堆词

![[AI引论/AIimg/img4/image-5.png|484x326]]

实际上，由于分母一样，可以只计算分子，比较两者大小
算概率再按比例分配一下就行

算分子时，假设词间条件独立，就变成了每个词的条件概率的乘积

![[AI引论/AIimg/img4/image-6.png]]
其中训练样本可以由词袋模型得到

但这种算法存在一个问题，如果一个单词在训练样本中从来没有出现过，那整个概率就会直接变成 0 ，这显然是不准确的

可以采用**加法平滑 additive smoothing** 
给统计得到的每种样本数量加上一个固定值 $\alpha$ 
相当于加了一份基础的训练样本，正负面情感中每个词均出现了 $\alpha$ 个
对于分母，每种样本都加了 $\alpha$ ，所以总共加了词汇类别数 × $\alpha$
![[AI引论/AIimg/img4/image-7.png|531x87]]
- 当 $\alpha=1$ 时，称为**拉普拉斯平滑**

## 信息检索指标 tf-idf

如何根据用户的输入找到相关文档

**词频 tf** ：某个词在所有文档中出现的频率
$$tf = \frac{n}{N} \quad \text{或} \quad tf = \log_{10}(n+1)$$
- $n$: 某个词在这个文档中出现的次数
- $N$: 这个文档中所有词出现的次数之和

但是不能只根据词频查找，因为很多词频高的词不一定代表这个文档的主题
比如说介词冠词，或者常用的名词动词

**逆文档频率 idf**：衡量某个词在所有文档中的罕见程度
$$idf = \log_{10}\frac{D}{1+d}$$
$D$: 语料库中包含的文档总数量
$d$: 语料库中出现某个词的文档数量

这个修正项可以弱化常见词，保留重要的词
比如，一个词在一个文档里高频出现，但在整个语料中出现频率低
那这就说明这个词可能是这个文档的关键词

## n-gram与马尔可夫模型

Markov链：无后效性，一个状态后的决策只与当前状态有关, 与历史状态无关
n-gram ：n 个连起来的token
马尔可夫模型：基于 n-gram ，按照概率对下一个词进行预测，从而生成文本

但是这种方式生成的文本看起来可能狗屁不通

# 基于神经网络的 NLP 模型 

## 词嵌入 embedding

词嵌入是一类技术，将一个词映射到一个向量，即想用向量描述一个词

**one-hot 编码**
句子中这个词出现的位置，比如：
"He wrote a book."
- he: `[1, 0, 0, 0]`  
- wrote: `[0, 1, 0, 0] ` 
- a: `[0, 0, 1, 0]`  
- book: `[0, 0, 0, 1]`

问题在于这样会很稀疏，而且没有考虑词语之间的语义相似性
所以不能直接当作词向量

实际处理时，可以用 1 对应的 index 代表这个 one-hot 向量

**分布式表示**
基本假设：上下文相似的词，其语义也相似
所以对于每一个词，只需要描述与上下文的关系即可

想把 one-hot 编码转换为词向量，有以下方法
- Embedding Layer ：一个简单的神经网络层
- word2vec ：基于局部上下文窗口，使用神经网络优化
- GloVe ：基于全局统计信息建模，适用于更大规模和稀疏语料

### Embedding Layer

对于输入的 one-hot 向量，用 1 的下标 $x_i$ 表示
Embedding Layer 将 $x_i$ 右乘权重矩阵 W， W 由学习得到
one-hot 向量右乘矩阵，即把矩阵中对应的一行取出来，而不用真的进行矩阵乘法，表示为
$$Embedding(x_i)=W[x_i]$$
这就是这个词对应的**嵌入向量**
对于一个包含多个索引的输入序列 $x = [x_1, x_2, ..., x_n]$
$$\text{Embedding}(x) = \begin{bmatrix}W[x_1] \\W[x_2] \\\vdots \\W[x_n]\end{bmatrix} \in \mathbb{R}^{n \times D}$$

### word2vec

模型类型
- **CBOW (Continuous Bag of Words) 连续词袋模型**
    - 目标：根据上下文预测中心词
    - 输入：上下文词汇（窗口中的前后单词）
    - 输出：当前目标词
- **Skip-Gram**
    - 目标：根据目标词预测上下文
    - 输入：目标词
    - 输出：上下文词（窗口中的前后单词）

**CBOW 模型**

![[AI引论/AIimg/img4/image-8.png|263x292]]

- 输入：上下文中若干个词的One-hot表示
- 输出：当前中心词对应的词向量

具体过程

1. 输入层
    - 输入上下文的词的 one-hot 编码
    - 假设词汇表大小为V，输入是C×V的矩阵（C为上下文词的数量）
2. 词向量映射
    - 每个 one-hot 编码会与同一个权重矩阵 周围词向量矩阵 $W_{V×N}$ 相乘
    - 每个上下文词会被映射到一个 1×N 的低维向量
3. 求平均
    - 将全部上下文词的向量取均值，表示上下文的整体语义
4. 输出层
    - 将这个 1×N 的隐藏向量与另一个矩阵 中心词向量矩阵 $W'_{N \times V}$ 相乘
    - 得到一个1×V 的向量，即中心词对应的词向量
5. Softmax 与反向传播
    - 对输出向量做 Softmax 归一化，得到概率分布，表示词汇表中每个词是“中心词”的概率
    - 用真实中心词对应的 one-hot 编码作为 ground-truth，求交叉熵损失，调整模型权重

训练后，相似的词会逐渐聚集，可以用词向量的欧氏距离反映语义相似度
甚至可以出现一些高级的相似性，比如  'man' - 'woman' $\approx$  'king' - 'queen'

缺点
- 静态性
	- 无论词在句子里出现的上下文如何，词向量是从固定的语料得出的，所以是固定的，可能无法处理一些多义情况
- 丢失顺序信息
	- 在上下文聚合（即处理多个上下文单词的词向量）时，采用的是简单的加和或取平均值操作，这就导致了词序丢失
	- CBOW 假设上下文单词是独立的，也就是说，它认为上下文中的单词以“袋子”的形式存在

### FFNN 前馈神经网络 (feedforward)

FFNN 的主要特征是数据从网络的一端流入，经过处理后从另一端流出，信息只在层与层之间向前传播，而不会出现回馈或循环
MLP 就是一种 FFNN，而像后面的 RNN，参数是循环传递的，就不是

以下面的一种 FFNN 为例
文本情感分类应用：基于word2vec/GloVe的文本特征
考虑一个简单的模型

![[AI引论/AIimg/img4/image-9.png]]
- 输入层：一段文本，由多个单词组成
	- 文本中的每个词，通过词嵌入得到对应的词向量，维数为 d
- 池化层
	- 可以采用 sum avg 等 pooling 操作
	- 可以使用 concat 将不同的 pooling 结果连起来
	- 将每个词的词向量汇总成一个向量 x ，表示整段文本的特征
$$\mathbf{x} = \frac{1}{n} \sum_{i=1}^{n} \mathbf{e}(w_i)$$
- 隐藏层
	- 使用 Linear + activation function 将 x 变换为 h
	-  W 用于提取高维特征
$$\mathbf{h} = \sigma(\mathbf{W}\mathbf{x} + \mathbf{b})$$
- Softmax 与输出层
	- 将隐藏层向量 h 通过权重矩阵 U 进行变换得到 z ，将隐藏层的高维表示进一步映射到目标空间，从而得到概率分布
	-  U 维度为 C×$d_h$，其中 `C` 是分类标签的数量，如情感分析可能有 3 类，正面、中性、负面 
	-  z 是一个 C×1 的向量，表示对每个类别的打分
$$\mathbf{z} = \mathbf{U}\mathbf{h}$$
	- 对 z 使用 softmax 函数得到概率分布
$$\hat{\mathbf{y}} = \text{softmax}(\mathbf{z})$$

## RNN 递归神经网路

普通神经网络（如MLP）对输入数据几乎只做单次独立计算，无法处理序列化数据（例如文本、语音、一系列时间序列历史数据），如果用它处理上一时间点的输出结果或历史上下文关系，它完全没概念，缺乏记忆

RNN 由以下基本结构组成

![[AI引论/AIimg/img4/image-11.png|148x187]]![[AI引论/AIimg/img4/image-12.png|489x187]]

RNN“递归”的本质是，当前时刻 t 的输出不仅依赖于当前的输入 $x_t$ ，还依赖于先前时刻计算的状态 $h_{t-1}$，这种方式叫**自回归生成**

对于输入的句子，生成每一个单词的词向量 $x_i$ 

首先，计算当前时刻的隐藏状态 $h_t$
$$h_t=f(W_th_{t-1}+W_xx_t+b_h)$$
- $W_h$  $W_x$ $b_h$ 在不同时间步共享
- $f$ 为激活函数（如 tanh relu）
- $h_t$ 为时刻 t 的隐藏状态，用于
	- 生成当前时刻的输出
	- 传递到下一时间步，代表记忆信息

而后，如果当前时刻需要输出，就可以由 $h_t$ 得到
$$y_t=g(W_yh_t+b_y)$$
- $W_y$  $b_y$ 在不同时间步共享
- $g$ 为激活函数（如 softmax）
- $y_t$ 为时刻 t 的输出（如分类概率，可进一步得出对应单词）

当所有词向量都被处理后，就停止循环，进行输出

优点
- 能够处理任意长度数据
- 即使输入变长，模型大小依旧不变 

缺点
- 未来状态的计算依赖于过去的状态，无法并行计算
- 很难处理长距离依赖，会发生梯度衰减

## Seq2Seq

Seq2Seq，序列到序列转换的模型架构
- 输入和输出为序列，且长度可能不同
- 最经典的应用场景是机器翻译

Seq2Seq 模型由两大部分组成
- **编码器 Encoder**
    - 将输入序列（源语言）编码成一个固定长度的向量，称为**上下文向量context vector** ，表示输入序列的语义
    - 这个语义表示捕获了输入序列的全局语义信息
- **解码器 Decoder**
    - 解码器根据编码器生成的上下文表示，逐步生成输出序列（目标语言）
    - 解码器是一个自回归模型，依赖已经生成的部分（目标序列的前缀）来推测下一个输出词
    - 当预测到特殊符号 EOS 时，即为终止

### 注意力机制 attention

当我们阅读句子时，我们不会同时关注所有词
例如，在句子“I love deep learning because it is powerful”中，理解“it”时，我们自然会把注意力放到“deep learning”上，而非其他单词
注意力机制试图模仿人类的这种能力，通过一个权重分布，告诉模型在当前输入中哪些部分更重要

![[AI引论/AIimg/img4/image-13.png]]

**自注意力机制 Self Attention**
表示“加权平均”的过程，通常分为三个元素
- Query 查询
- Key 键
- Value 值

先从输入得到 Q K V
假设输入是一个由 $n$ 个单词组成的句子或序列
经过 Embedding Layer ，就可以得到
$$ 
X = [x_1, x_2, ..., x_n] \in \mathbb{R}^{n \times d_\text{model}}\\
$$
- $x_i \in \mathbb{R}^{d_\text{model}}$ 表示句子中第 $i$ 个词的嵌入向量

将 $X$ 分别右乘权重矩阵，得到 $Q, K, V$
$$
\begin{align}
Q = XW^Q, \quad K = XW^K, \quad V = XW^V \\
Q \in \mathbb{R}^{n \times d_k}, \quad K \in \mathbb{R}^{n \times d_k}, \quad V \in \mathbb{R}^{n \times d_v}
\end{align}
$$
- $W^Q,\ W^K \in \mathbb{R}^{d_\text{model} \times d_k}$ 和  $W^V \in \mathbb{R}^{d_\text{model} \times d_v}$ 由学习得到
- 一般取 $d_k = d_\text{model}$

注意力的目标是：用当前查询Q 找到最匹配的键 K，并据此获得对应的值 V
$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right) \mathbf{V}
$$
- $QK^T$ 做点积，表示 Q 和每个 K 的匹配程度
- 除以 $\sqrt d_k$​​ 防止乘积过大导致梯度爆炸
- softmax 归一化，作为权重，确保总和为 1
- 用 softmax 的结果乘以 V，即加权求和

**多头注意力 Multi-Head Attention**
对同一个输入序列进行多次独立的注意力计算，每个注意力头 attention head 可以关注输入序列的不同特征
这时候，取 $d_k = d_\text{model} / h$， $h$ 是多头注意力中的头数

### Transformer

注意力机制并没有像 RNN 模型那样的天然顺序性
- 在 RNN 中，输入序列是在时间步中逐步处理的，每个时间步的隐藏状态天然代表了位置信息
- 但在 Transformer 中，所有的输入向量是同时被处理的，所以模型无法直接感知输入序列中单词的顺序或位置
所以，一般会在每个单词的嵌入向量上叠加一个**位置编码 positional encoding**

![[AI引论/AIimg/img4/image-14.png]]

位置编码可以通过学习得到，也可以用正余弦表示直接得到

$$
\begin{align}
\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right) \\
\\
\text{PE}(pos, 2i + 1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
\end{align}
$$
- $pos$ 代表一个单词在句子中的位置（从 0 开始）
- $d$ 为嵌入向量的长度，位置编码的长度与其相同
- $\text{PE}(pos, i)$ 表示在 pos 位置的单词，其位置编码的第 i 个元素，偶数位置正弦，奇数位置余弦

为啥不直接用 one-hot 编码？因为这只能提供绝对的位置信息
正余弦编码提供了更多信息，包括
- 绝对位置的信息：每个单词的正余弦编码是固定的，显式表示其具体位置
- 相对位置的信息：正余弦编码的周期性特点能够让单词之间的位置差异直接体现在编码的相似性上

比如，对于 pos 0~9，嵌入向量为 64 维，其位置编码长这样

![[AI引论/AIimg/img4/image-16.png]]

在 Encoder 中，一般还会使用 skip link 与 layer normalization

![[AI引论/AIimg/img4/image-15.png|449x408]]

Add 代表 skip link
Normalize 代表层归一化
$$ 
\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} * \gamma + \beta 
$$
稳定数据分布，能加快训练速度，提高训练稳定性

由2个 Encoder 与2个 Decoder 组成的Transformer

![[AI引论/AIimg/img4/image-17.png]]

其中由 Encoder 指向Decoder 的虚线代表 Encoder-Decoder Attention
- Q 来自于 Decoder 上一层输出
- K V 来自于 Encoder 输出
- Q K V 对应的权重矩阵都在 Decoder

Encoder 输出维数为 $B×n×d_\text{model}$ 
- B 为 batch 大小
- n 为句子长度，一般会将同一 batch 的句子补成相同长度
- $d_\text{model}$ 为嵌入向量的长度，是超参

实际实现时，还会使用**注意力掩码 attention mask**
$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}  + \mathbf{M} \right) \mathbf{V}
$$
- M 为 mask，屏蔽的部分填充 $-\infty$ 

Padding Mask
- 输入序列：包含填充符的序列，例如 `[A, B, <pad>, <pad>]`
$$
M_{pad} = \begin{bmatrix} 0 & 0 & -\infty & -\infty \\ 0 & 0 & -\infty & -\infty \\ -\infty & -\infty & -\infty & -\infty \\ -\infty & -\infty & -\infty & -\infty \end{bmatrix}
$$
- 作用：在 Encoder 的 self-attention 中，对于输入的补长的句子，用 mask 忽视补长的部分，因为这部分没有实际含义

Causal Mask
- 生成序列：自回归生成的序列，例如生成第3步时输入为 `[<s>, Y1, Y2]`
$$
M_{\text{causal}} = \begin{bmatrix} 0 & -\infty & -\infty \\ 0 & 0 & -\infty \\ 0 & 0 & 0 \end{bmatrix}
$$
- 作用：在 Decoder 的 self-attention 中，由于Decoder 是自回归生成的，防止训练与测试时 Decoder 提前获取后续的单词信息，确保生成 $Y_i$ 时仅依赖前面的信息 $Y_1,Y_2,...,Y_{i-1}$

一些情况下，甚至可以不用 Encoder，只用 Decoder，比如 GPT模型


# Summary

1. 统计方法起步
	- BoW → TF-IDF → n-gram → 马尔可夫模型
	- 核心是统计概率建模
2.  神经网络引入
	- Word2Vec → RNN → Seq2Seq 
	- 端到端学习，注意力机制，语义理解增强
3. 现代 NLP
	- Transformer → BERT → GPT (大模型崛起)
	- 自注意力、多头注意力机制，大规模预训练，泛任务适应性


























