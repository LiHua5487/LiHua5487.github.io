---
date: 2025-03-24
tags:
  - AI
---
# åŸºæœ¬æ¦‚å¿µ

- æ¨¡å‹(Model)ï¼šä¸€ä¸ªåŒ…å«å‚æ•°(parameters) çš„å‡½æ•° 
- æ ‡ç­¾(Label)ï¼šè¦é¢„æµ‹çš„ç±»åˆ«æˆ–æ•°å€¼
- æ¨¡å‹è®­ç»ƒ/å­¦ä¹ (training/learning)ï¼šé€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°æ¥æ‹Ÿåˆ (fit) è®­ç»ƒæ•°æ® (training data)

- æ¨¡å‹åˆ†ç±»
![[AIimg/img2/image.png]]

- éå‚æ•°åŒ–æ¨¡å‹ï¼š
	- æ¨¡å‹ä¸èƒ½è¢«æœ‰é™å‚æ•°å®šä¹‰ï¼Œæˆ–ä¸åŒ…å«å‚æ•°
	- éœ€è¦ä¿ç•™è®­ç»ƒæ ·æœ¬ï¼Œä»¥å¯¹æµ‹è¯•æ ·æœ¬åšå‡ºé¢„æµ‹ 
- å‚æ•°åŒ–æ¨¡å‹
	- æ¨¡å‹åŒ…å«å¯è®­ç»ƒçš„å‚æ•°ï¼Œé€šè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®æ¥ä¼°ç®—æ¨¡å‹å‚æ•° 
	- è®­ç»ƒå¥½æ¨¡å‹å‚æ•°åï¼Œå¯ä»¥ä¸¢å¼ƒè®­ç»ƒæ•°æ®ï¼Œä»…ä¾é æ¨¡å‹å‚æ•°å»é¢„æµ‹æ–°æ ·æœ¬ 

- å›å½’(regression): æ ‡ç­¾ (label) æ˜¯è¿ç»­ (continuous) å€¼ ï¼ˆå¦‚ é¢„æµ‹æˆ¿ä»·ï¼‰ 
- åˆ†ç±»(classification): æ ‡ç­¾ (label) æ˜¯ç¦»æ•£ (discrete) å€¼ ï¼ˆå¦‚ åˆ¤æ–­ç‹—çš„å“ç§ï¼‰

- è®­ç»ƒè¯¯å·®(training error)ï¼šåœ¨è®­ç»ƒé›†ä¸Šçš„å¹³å‡è¯¯å·® 
	- é€šè¿‡æœ€å°åŒ–è®­ç»ƒè¯¯å·®æ¥è®­ç»ƒæ¨¡å‹ 
	- å¯¹åˆ†ç±»é—®é¢˜ï¼Œé€šå¸¸ç”¨é”™è¯¯ç‡æ¥è¡¡é‡è®­ç»ƒè¯¯å·®
- æµ‹è¯•è¯¯å·®(test error)ï¼šåœ¨æµ‹è¯•é›†ä¸Šçš„å¹³å‡è¯¯å·®
	- è®­ç»ƒå®Œæˆåï¼Œç”¨æ¥çœŸæ­£è¡¡é‡æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„å¥½å 
	- è¡¡é‡æ¨¡å‹çš„æ³›åŒ–(generalization) èƒ½åŠ› 
	- è®­ç»ƒè¯¯å·®ä½ä¸ä»£è¡¨æµ‹è¯•è¯¯å·®ä¸€å®šä¹Ÿä½ï¼ˆè¿‡æ‹Ÿåˆï¼‰ 
- è®­ç»ƒé›†(training set)ã€æµ‹è¯•é›†(test set) åˆ’åˆ† 
	- ç»™å®šå…¨éƒ¨çš„æ•°æ®ï¼ŒæŒ‰ä¸€å®šæ¯”ä¾‹éšæœºåˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
	- æŒ‰ç…§æ—¶é—´é¡ºåºæ’åˆ—ï¼Œå†æŒ‰å…ˆååˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†

- è¿‡æ‹Ÿåˆ(overfitting)ï¼šæµ‹è¯•è¯¯å·®è¿œè¿œå¤§äºè®­ç»ƒè¯¯å·® 
	- é”™æŠŠè®­ç»ƒæ ·æœ¬ä¸­æ‰¾åˆ°çš„ç‰¹æ®Šè§„å¾‹å½“åšäº†æ™®éè§„å¾‹ï¼Œåº”é¿å…è¿™ç§ç°è±¡
- æ¬ æ‹Ÿåˆ(underfitting)ï¼šè®­ç»ƒå®Œæˆåï¼Œè®­ç»ƒè¯¯å·®ä»ç„¶å¾ˆå¤§
	- è¯´æ˜è¿è®­ç»ƒæ ·æœ¬éƒ½æ²¡æœ‰æ‹Ÿåˆå¥½

è®­ç»ƒè¿‡ç¨‹ä¸­ä¸€èˆ¬æ¶‰åŠä¸€äº›è¶…å‚æ•°(hyperparameters)
- æ¢¯åº¦ä¸‹é™çš„å­¦ä¹ ç‡ ğ›¼ï¼Œæ­£åˆ™åŒ–é¡¹çš„ç³»æ•° ğœ† ç­‰
- æ­¤å¤–ï¼Œä½¿ç”¨å“ªç§æ¨¡å‹ã€å“ªç§æŸå¤±å‡½æ•°ã€å“ªç§æ­£åˆ™åŒ–ç­‰ä¹Ÿå¯çœ‹ä½œè¶…å‚æ•° 

ä½¿ç”¨éªŒè¯é›†(validation set) æ¥é€‰æ‹©æœ€ä¼˜è¶…å‚æ•°
- åœ¨è®­ç»ƒé›†ã€æµ‹è¯•é›†ä¹‹å¤–å¼•å…¥éªŒè¯é›†ï¼Œé€šå¸¸æŒ‰ç…§è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†åˆ†åˆ« 80%/10%/10%çš„æ¯”ä¾‹åˆ’åˆ† 
- å¯¹æ¯ä¸€ç»„è¶…å‚æ•°çš„ç»„åˆï¼Œåœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒå‚æ•°ï¼Œåœ¨éªŒè¯é›†ä¸ŠéªŒè¯æ¨¡å‹è¯¯å·®
- éå†æ‰€æœ‰å¯èƒ½çš„è¶…å‚æ•°ç»„åˆï¼Œé€‰æ‹©åœ¨éªŒè¯é›†ä¸Šè¯¯å·®æœ€å°çš„æ¨¡å‹å’Œè¶…å‚æ•°
- ä½¿ç”¨é€‰å®šçš„æ¨¡å‹å’Œè¶…å‚æ•°åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆæµ‹è¯•æ¨¡å‹è¯¯å·®ï¼Œä¼°è®¡æ¨¡å‹æ³›åŒ–èƒ½åŠ›

# K-æŠ˜äº¤å‰éªŒè¯

æœ‰æ—¶æ€»æ•°æ®é‡æ¯”è¾ƒå°‘ï¼Œ10%çš„éªŒè¯é›†ä¸è¶³ä»¥å‡†ç¡®åœ°éªŒè¯æ¨¡å‹æ€§èƒ½

K-æŠ˜äº¤å‰éªŒè¯
- å°†æµ‹è¯•é›†å¤–çš„æ‰€æœ‰æ•°æ®éšæœºåˆ†ä¸º K ä»½ 
- å¯¹ä¸€ç»„ç»™å®šçš„è¶…å‚æ•°ç»„åˆï¼š 
	- æ¯æ¬¡ä½¿ç”¨ K-1 ä»½æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œç”¨1ä»½éªŒè¯æ¨¡å‹è¯¯å·® 
	- å°†Kæ¬¡éªŒè¯çš„æ¨¡å‹è¯¯å·®å–å¹³å‡æ¥è¡¡é‡è¯¥ç»„è¶…å‚æ•°çš„å¥½å 
	- éå†æ‰€æœ‰å¯èƒ½çš„è¶…å‚æ•°ç»„åˆ 
- è¿”å›å¹³å‡è¯¯å·®æœ€ä½çš„è¶…å‚æ•°ç»„åˆï¼Œåœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆæµ‹è¯•æ¨¡å‹æ€§èƒ½

# Empirical Risk Minimization (ERM)

- ç¡®å®šæ¨¡å‹ $f(x)$
- ç¡®å®šæŸå¤±å‡½æ•° $L(f(x),y)$
- åœ¨è®­ç»ƒé›†ä¸Šæœ€å°åŒ–æŸå¤±å‡½æ•°å‡å€¼
$$
\min_{\theta}\frac{1}{n} \sum_{i \in [n]} L(f(x_i), y_i)
$$
ä¸€èˆ¬å¯ä»¥ç”¨GDä¼˜åŒ–ï¼Œå¯¹äºç›‘ç£å¼å­¦ä¹ ï¼Œè¿™ä¸ªæµç¨‹æ˜¯é€šç”¨çš„ï¼Œå…·ä½“å·®åˆ«åœ¨äºmodelä¸Lossçš„é€‰å–

# k-ä¸´è¿‘ç®—æ³•

å¯¹äºä¸€ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œç”¨è®­ç»ƒæ ·æœ¬ä¸­è·ç¦»å®ƒæœ€è¿‘çš„kä¸ªæ ·æœ¬ä¸­å å¤šæ•°çš„æ ‡ç­¾ æ¥é¢„æµ‹æµ‹è¯•æ ·æœ¬

![[AIimg/img2/image-1.png|700x233]]

åŠ æƒçš„ k-NN ï¼šå¯¹äºä¸åŒçš„è·ç¦»ï¼Œå¯ä»¥å¯¹åº”ä¸åŒçš„æƒé‡ï¼Œè·ç¦»è¶Šè¿‘è´¡çŒ®è¶Šå¤§ï¼Œè®¡ç®—ä¸åŒç±»åˆ«çš„æƒé‡å’Œï¼Œå–å¤§çš„ä¸€ä¸ª

- ä¼˜ç‚¹
	- æ²¡æœ‰ä»»ä½•å†…ç½®å‚æ•°ï¼Œä¸éœ€è¦è®­ç»ƒ 
	- åªéœ€è¦ä¸€ä¸ªè·ç¦»å‡½æ•°å³å¯ï¼ˆé»˜è®¤ä¸ºæ¬§æ°è·ç¦»ï¼‰

- ç¼ºç‚¹
	- éœ€è¦å­˜å‚¨æ‰€æœ‰è®­ç»ƒæ ·æœ¬ 
	- åœ¨æµ‹è¯•æ—¶éœ€è¦è®¡ç®—æµ‹è¯•æ ·æœ¬åˆ°æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦» 
	- æœ‰æ—¶å¾ˆéš¾æ‰¾åˆ°ä¸€ä¸ªå¥½çš„è·ç¦»å‡½æ•°ï¼Œæˆ–ä¸ä¸€å®šèƒ½å‡†ç¡®åæ˜ ç›¸ä¼¼åº¦
	- åœ¨é«˜ç»´åº¦æ—¶ï¼Œä¼šå‘ç”Ÿ**ç»´åº¦ç¾éš¾ Curse of Dimensionality**

![[AIå¼•è®º/AIimg/img2/image-4.png]]

# çº¿æ€§å›å½’

æ¨¡å‹
$$
f(x) = w^T x + b
$$
- è¾“å…¥: $x \in \mathbb{R}^d$
- å‚æ•°: $w \in \mathbb{R}^d, b \in \mathbb{R}$ï¼Œæƒé‡ (weight) å’Œ åç½® (bias)
- è¾“å‡º: $f(x) \in \mathbb{R}$

æŸå¤±å‡½æ•°
- å¹³æ–¹æŸå¤±å‡½æ•° (squared loss, L2 loss)
$$
J(w, b) = \frac{1}{n} \sum_{i \in [n]} L(f(x_i), y_i) = \frac{1}{n} \sum_{i \in [n]} (w^T x_i + b - y_i)^2
$$
- ä¼˜åŒ–æ–¹æ³•ï¼šæ¢¯åº¦ä¸‹é™è®­ç»ƒ
- æ€§è´¨ï¼šå‡¸å‡½æ•°ï¼ˆå±€éƒ¨æœ€å°å€¼å³ä¸ºå…¨å±€æœ€å°å€¼ï¼‰

å¯¹äº L2 Lossï¼Œæœ‰
$$
\begin{align}
J(w, b) = \frac{1}{n} \sum_{i \in [n]} L(f(x_i), y_i) &= \frac{1}{n} \sum_{i \in [n]} (w^T x_i + b - y_i)^2 = \frac{1}{n} \sum_{i \in [n]} e_i^2 \\
where\ e_i &= w^T x_i + b - y_i
\end{align}
$$
æ³¨æ„åˆ°ï¼ŒL2 Loss æ˜¯å‡¸å‡½æ•°ï¼Œå…¶æ¢¯åº¦
$$
\begin{align}
\frac{\partial J(w, b)}{\partial w} &= \frac{2}{n} \sum_{i \in [n]} (w^T x_i + b - y_i) x_i \in \mathbb{R}^d \\
\frac{\partial J(w, b)}{\partial b} &= \frac{2}{n} \sum_{i \in [n]} (w^T x_i + b - y_i) \in \mathbb{R}
\end{align}
$$
æ®æ­¤æ›´æ–°å‚æ•°
$$
\begin{align}
w &\leftarrow w - \alpha \cdot \frac{2}{n} \sum_{i \in [n]} e_i x_i \\
b &\leftarrow b - \alpha \cdot \frac{2}{n} \sum_{i \in [n]} e_i
\end{align}
$$

è®­ç»ƒè¿‡ç¨‹ï¼š
1. ç»™å®šè®­ç»ƒæ•°æ® $\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$ï¼Œå­¦ä¹ ç‡ $\alpha$
2. åˆå§‹åŒ–æ¨¡å‹å‚æ•° $w, b$
3. è®­ç»ƒæ¨¡å‹ï¼š
	- è®¡ç®—æ¨¡å‹é¢„æµ‹å€¼ 
	  $\hat{y}_i = w^T x_i + b, \ \forall i \in [n]$  
	- è®¡ç®—å¹³æ–¹æŸå¤±å‡½æ•°  
	  $J(w, b) = \frac{1}{n} \sum_{i \in [n]} (\hat{y}_i - y_i)^2$  
	- è®¡ç®—æ¢¯åº¦  
	  $\frac{\partial J(w, b)}{\partial w} = \frac{2}{n} \sum_{i \in [n]} (\hat{y}_i - y_i) x_i$  $\frac{\partial J(w, b)}{\partial b} = \frac{2}{n} \sum_{i \in [n]} (\hat{y}_i - y_i)$  
	- æ¢¯åº¦ä¸‹é™æ›´æ–° $w, b$  
	  $w \gets w - \alpha \cdot \frac{\partial J(w, b)}{\partial w}, \ b \gets b - \alpha \cdot \frac{\partial J(w, b)}{\partial b}$  
	- é‡å¤ä»¥ä¸Šæ­¥éª¤ï¼Œç›´åˆ°æŸå¤±å‡½æ•°ä¸å†ä¸‹é™æˆ–è¾¾åˆ°é¢„è®¾è¿­ä»£æ¬¡æ•°  
4. ç»™å®šæ–°æ•°æ® $x$ï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹å…¶æ ‡ç­¾  $\hat{y} = w^T x + b$

# é€»è¾‘å›å½’

- åº”ç”¨ï¼šå¤„ç†äºŒåˆ†ç±»é—®é¢˜
- æ¨¡å‹ï¼šçº¿æ€§æ¨¡å‹ $f(x) = w^Tx+b$  ï¼Œæœ€åç”¨æ¿€æ´»å‡½æ•°åŒ–ä¸ºæ¦‚ç‡
- æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µæŸå¤± cross entropy loss / logistic loss

## äºŒåˆ†ç±»é—®é¢˜

æ ‡ç­¾åªæœ‰ä¸¤ç§
  - $y \in \{-1, 1\}$ï¼Œ$-1$ ä»£è¡¨è´Ÿç±» (negative class)ï¼Œ$1$ ä»£è¡¨æ­£ç±» (positive class)
  - ä¾‹å¦‚åƒåœ¾é‚®ä»¶è¯†åˆ«ä¸­ï¼Œ$1$ ä»£è¡¨åƒåœ¾é‚®ä»¶ï¼Œ$-1$ ä»£è¡¨æ­£å¸¸é‚®ä»¶

ä¸€èˆ¬ä¸ç›´æ¥è®© $f(x) \in \mathbb{R}$ æ‹Ÿåˆ $y \in \{-1,1\}$
  - ä¸ºäº†å°†å®æ•°è¾“å‡º $f(x)$ è½¬æ¢ä¸ºç±»åˆ« $\{-1, 1\}$ï¼Œé€šå¸¸é‡‡ç”¨ $\text{sign}$ å‡½æ•°ï¼š
$$
\text{sign}(f(x)) = 
\begin{cases} 
1, & \text{if } f(x) > 0 \\
-1, & \text{if } f(x) < 0
\end{cases}
$$

ä½¿ç”¨ä»€ä¹ˆæŸå¤±å‡½æ•°ï¼Ÿ
  - æœ€ç›´æ¥çš„ç›®æ ‡æ˜¯æœ€å°åŒ–åˆ†ç±»é”™è¯¯æ•°ï¼Œé‡‡ç”¨**é›¶ä¸€æŸå¤±å‡½æ•° (zero-one loss)**ï¼š

$$
 L(f(x), y) = 
\begin{cases} 
 0, & \text{if } \text{sign}(f(x)) = y \quad \text{(åˆ†ç±»æ­£ç¡®)} \\
1, & \text{if } \text{sign}(f(x)) = -y \quad \text{(åˆ†ç±»é”™è¯¯)} \end{cases}
$$
  æˆ–ç­‰ä»·åœ°å†™ä½œï¼š
$$
L(f(x), y) = 
\begin{cases} 
0, & \text{if } y \cdot f(x) \geq 0 \\
1, & \text{if } y \cdot f(x) < 0
\end{cases}
$$

![[AIimg/img2/image-2.png|449x175]]

- é—®é¢˜ï¼šé›¶ä¸€æŸå¤±å‡½æ•°çš„ç¼ºç‚¹
  - é›¶ä¸€æŸå¤±å‡½æ•°æ˜¯é˜¶è·ƒå‡½æ•°ï¼Œä¸å¯å¾®ä¸”ä¸è¿ç»­ 
  - æ¢¯åº¦ä¸‹é™æ— æ³•ä¼˜åŒ–ï¼š
    - åœ¨ $f(x) = 0$ æ—¶ä¸å¯å¾®
    - å…¶ä½™å¤„æ¢¯åº¦éƒ½ä¸º $0$ï¼Œæ— æ³•æä¾›ä¸‹é™æ–¹å‘

## MLE ä¸ äº¤å‰ç†µæŸå¤±

å¯¹è§‚æµ‹æ•°æ®è¿›è¡Œ (æ¡ä»¶) æ¦‚ç‡å»ºæ¨¡
- å¯¹æœºå™¨å­¦ä¹ ï¼Œæ¯ä¸ªè§‚æµ‹æ•°æ®å³ä¸€ä¸ªè®­ç»ƒæ ·æœ¬
- å¯¹åˆ¤åˆ«å¼æ¨¡å‹ï¼Œæˆ‘ä»¬åªå»ºæ¨¡ $p(y|x; \theta)$, $\theta$ ä¸ºæ¨¡å‹å‚æ•°
- é€šè¿‡æœ€å¤§åŒ–è§‚æµ‹æ•°æ®åœ¨ç»™å®šæ¦‚ç‡æ¨¡å‹ä¸‹çš„ä¼¼ç„¶ï¼ˆä¾‹å¦‚ï¼šæŠŠè®­ç»ƒæ ·æœ¬é¢„æµ‹ä¸ºæ­£ç¡®æ ‡ç­¾çš„**æ¦‚ç‡**ï¼‰æ¥ä¼°è®¡æ¨¡å‹å‚æ•°
å¦‚æœè®­ç»ƒæ ·æœ¬ç›¸äº’ç‹¬ç«‹ï¼ˆç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾ï¼‰ï¼Œåˆ™æœ€å¤§ä¼¼ç„¶ä¼°è®¡å¯å†™ä¸º  
$$
\max_{\theta} \prod_{i \in [n]} p(y = y_i | x = x_i; \theta)
$$
æˆ–ç®€å†™ä¸º  
$$
\max_{\theta} \prod_{i \in [n]} p(y_i | x_i; \theta)
$$
ä½†æ˜¯ï¼Œå¤§é‡æ¦‚ç‡è¿ä¹˜å®¹æ˜“é€ æˆæ•°å€¼è¶…å‡ºè®¡ç®—ç²¾åº¦
æ‰€ä»¥é‡‡ç”¨æœ€å¤§åŒ–**å¯¹æ•°ä¼¼ç„¶ (log-likelihood)**  
$$
\max_{\theta} \log\left(\prod_{i \in [n]} p(y_i | x_i; \theta)\right) \iff \max_{\theta} \sum_{i \in [n]} \log(p(y_i | x_i; \theta))
$$

---

ä»¥æŠ›ç¡¬å¸ä¸ºä¾‹ï¼Œå‡è®¾æŠ› n æ¬¡ï¼Œæ­£é¢æœ‰ m æ¬¡ï¼Œä¼°è®¡æ­£é¢æ¦‚ç‡ p
$$
\begin{align}
Likelihood &= p^m(1-p)^{n-m} \\
log-L &= mlogp + (n-m)log(1-p)
\end{align}
$$
æœ€å¤§åŒ– $log-L$ï¼Œæ±‚å…¶æ¢¯åº¦ä¸º0çš„ç‚¹ï¼Œå¾—åˆ° $p=\frac{m}{n}$ ï¼Œç»æ£€éªŒæ˜¯å…¨å±€æœ€å¤§å€¼

---

å¯¹äºé€»è¾‘å›å½’ï¼Œå…ˆæŠŠçº¿æ€§æ¨¡å‹ $f(x)=w^T+b$ è½¬åŒ–ä¸ºæ¦‚ç‡
é‡‡ç”¨ sigmoid å‡½æ•°å½’ä¸€åŒ–
$$
\begin{align}
\sigma(z) &= \frac{1}{1 + e^{-z}} \\
1-\sigma(z) &= \frac{e^{-z}}{1 + e^{-z}} = \frac{1}{1 + e^{z}} = \sigma(-z)
\end{align}
$$
æ— è®º y å– 1 è¿˜æ˜¯ -1ï¼Œå‡æœ‰
$$
\begin{align}
p(y = 1 | x; \theta) &= \sigma(f(x)) = \sigma(y \cdot f(x)) \\
p(y = -1 | x; \theta) &= 1 - \sigma(f(x)) = \sigma(-f(x)) = \sigma(y \cdot f(x))
\end{align}
$$
å…¶ä¸­ $\theta$ ä¸º $(w,b)$ ï¼Œæ•…å¯é‡‡ç”¨ä¸‹å¼ä½œä¸ºæ¦‚ç‡
$$p(y | x; \theta)=\sigma(y \cdot f(x))$$
åˆ™ log-likelihood å¯è¡¨ç¤ºä¸º 
$$
\begin{align*}
\max_{w, b} \sum_{i \in [n]} \log \big[p(y_i | x_i; w, b)\big] 
&= \sum_{i \in [n]} \log \big[\sigma(y_i \cdot f(x_i))\big] \\
& = \sum_{i \in [n]} \log \big[\sigma(y_i(w^T x_i + b))\big] \\
&= \sum_{i \in [n]} \log \Bigg[\frac{1}{1 + e^{-y_i (w^T x_i + b)}}\Bigg] \\[10pt]
&= -\sum_{i \in [n]} \log \big[1 + e^{-y_i (w^T x_i + b)}\big] 
\end{align*}
$$
è½¬åŒ–ä¸º ERM çš„å½¢å¼ï¼Œå†™æˆæŸå¤±å‡½æ•°ï¼Œç§°ä¸º **äº¤å‰ç†µæŸå¤±**
$$
L(f(x_i),y_i) = \frac{1}{n} \sum_{i \in [n]} \log \big[1 + e^{-y_i (w^T x_i + b)}\big]
$$
- è¿™ä¸ªå¼å­æ˜¯äº¤å‰ç†µæŸå¤±å¯¹äºäºŒåˆ†ç±»é—®é¢˜çš„æƒ…å†µ
æ³¨æ„åˆ°ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°è¿ç»­å¯å¾®ï¼Œä¸”ä¸ºå‡¸å‡½æ•°ï¼Œå…¶æ¢¯åº¦
$$
\begin{align*}
    \frac{\partial J(w, b)}{\partial w} 
    &= -\frac{1}{n} \sum_{i \in [n]} \frac{e^{-y_i (w^T x_i + b)}}{1 + e^{-y_i (w^T x_i + b)}} \cdot y_i x_i \\[10pt]
    &= -\frac{1}{n} \sum_{i \in [n]} \Big[1 - p(y_i | x_i; w, b)\Big] y_i x_i \\[15pt]
    \frac{\partial J(w, b)}{\partial b} 
    &= -\frac{1}{n} \sum_{i \in [n]} \frac{e^{-y_i (w^T x_i + b)}}{1 + e^{-y_i (w^T x_i + b)}} \cdot y_i \\[10pt]
    &= -\frac{1}{n} \sum_{i \in [n]} \Big[1 - p(y_i | x_i; w, b)\Big] y_i
\end{align*}
$$
ç›´è§‚åœ°æ¥çœ‹ï¼Œæ ·æœ¬ $i$ é¢„æµ‹ä¸ºå…¶çœŸå®æ ‡ç­¾çš„æ¦‚ç‡è¶Šæ¥è¿‘1ï¼ˆè¯´æ˜å·²ç»å……åˆ†æ‹Ÿåˆï¼‰ï¼Œå®ƒå¯¹æ¢¯åº¦çš„è´¡çŒ®è¶Šå°ï¼Œå³ä¸å¤ªéœ€è¦è°ƒæ•´

æ³¨ï¼šé€»è¾‘å›å½’ä»æ˜¯çº¿æ€§æ¨¡å‹ï¼Œåªæ˜¯è®­ç»ƒæ—¶ä½¿ç”¨ sigmoid å‡½æ•°è¿›è¡Œéçº¿æ€§å¤„ç†

# Softmax å›å½’

å¯¹ K åˆ†ç±»é—®é¢˜ï¼Œè®­ç»ƒ K ä¸ªäºŒåˆ†ç±»å™¨ï¼ˆå¦‚ é€»è¾‘å›å½’ï¼‰
ç¬¬ k ä¸ªäºŒåˆ†ç±»å™¨å°†ç¬¬ k ç±»å½“æˆæ­£ç±»ï¼Œ å…¶ä½™æ‰€æœ‰ç±»åˆ«éƒ½å½“æˆè´Ÿç±»

é—®é¢˜æ˜¯ï¼Œå½“ç±»åˆ«æ•°Kéå¸¸å¤§æ—¶ï¼Œåˆ†åˆ«è®­ç»ƒ K ä¸ªäºŒåˆ†ç±»å™¨ä»£ä»·å¤ªé«˜
è€Œä¸”ï¼ŒKä¸ªäºŒåˆ†ç±»å™¨äº’ç›¸ç‹¬ç«‹ï¼Œæ— æ³•ç»Ÿä¸€æˆä¸€ä¸ªæ¨¡å‹

é‡‡ç”¨ Softmax å›å½’
- åº”ç”¨ï¼šKåˆ†ç±»é—®é¢˜ï¼š $y \in \{1, 2, \dots, K\} = [K], x \in \mathbb{R}^d$
- æ¨¡å‹ï¼šå…±åŒè®­ç»ƒ K ä¸ªæ¨¡å‹ $f_k(x) \in \mathbb{R}, k \in [K]$

è®­ç»ƒæ—¶ï¼Œè¦å°†æ¨¡å‹è¾“å‡º $f_k(x)$ è½¬åŒ–ä¸ºå–ç¬¬ $k$ ç±»çš„æ¦‚ç‡
- ä¸èƒ½ä½¿ç”¨ sigmoid å‡½æ•°ï¼Œå› ä¸ºéœ€æ»¡è¶³å½’ä¸€åŒ–æ¡ä»¶
$$\sum_{k \in [K]} p(y = k | x) = 1$$
- è§£å†³æ–¹æ³•ï¼šä½¿ç”¨ softmax å‡½æ•°
$$
p(y = k | x) = \frac{e^{f_k(x)}}{\sum_{j \in [K]} e^{f_j(x)}}
$$
- e æŒ‡æ•°çš„æ”¾å¤§æ•ˆåº”ä¼šä½¿å¾—å¦‚æœ $f_k(x) \gg f_j(x), \forall j \neq k$ï¼Œåˆ™ $p(y = k | x) \approx 1$

å‡è®¾ $f_k(x)$ çš„å‚æ•°ä¸º $\theta_k$ï¼Œæœ€å¤§åŒ–è®­ç»ƒé›† $\{(x_i, y_i) | i \in [n]\}$ çš„å¯¹æ•°ä¼¼ç„¶ï¼š $$ \max_{\{\theta_k\}} \sum_{i \in [n]} \log p(y_i | x_i) = \sum_{i \in [n]} \log \left[ \frac{e^{f_{y_i}(x_i)}}{\sum_{j \in [K]} e^{f_j(x_i)}} \right] $$ç­‰ä»·äºæœ€å°åŒ–äº¤å‰ç†µæŸå¤±ï¼š 
$$ \min_{\{\theta_k\}} -\frac{1}{n} \sum_{i \in [n]} \log \left[ \frac{e^{f_{y_i}(x_i)}}{\sum_{j \in [K]} e^{f_j(x_i)}} \right] = \frac{1}{n} \sum_{i \in [n]} \left( \log \left[ \sum_{j \in [K]} e^{f_j(x_i)} \right] - f_{y_i}(x_i) \right) $$

# æ­£åˆ™åŒ–

å®é™…ä¸­åœ¨æŸå¤±å‡½æ•°ååŠ ä¸€é¡¹æ­£åˆ™åŒ–é¡¹ä¸€èµ·ä¼˜åŒ–ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
$$
\min_{f} \frac{1}{n} \sum_{i \in [n]} L(f(x_i), y_i) + \lambda \cdot R(f)
$$
- $\lambda \cdot R(f)$ ç§°ä¸º**æ­£åˆ™åŒ–é¡¹** (regularization term)ï¼Œç”¨äºæƒ©ç½šè¿‡äºå¤æ‚çš„æ¨¡å‹
- $\lambda > 0$ æ˜¯ä¸€ä¸ªè¶…å‚æ•° (hyperparameter)ï¼Œéœ€è¦é¢„å…ˆæŒ‡å®šï¼Œä¸éšå‚æ•°ä¼˜åŒ–

å¸¸è§è¿‡æ‹ŸåˆåŸå› ï¼š

1. æŸå‡ ä¸ªç‰¹å¾ç»´åº¦ $j$ æ”¯é… (dominate) äº†é¢„æµ‹ï¼Œå³è¿™äº›ç»´åº¦çš„æƒé‡ $w_j$ è¿‡å¤§

- é‡‡ç”¨ **L2 æ­£åˆ™åŒ–** 
$$R(f) = \|w\|_2^2 = w^T w = \sum_{j \in [d]} w_j^2$$
- ä½œç”¨ï¼š$w_j^2$ ä¼šæ”¾å¤§è¾ƒå¤§çš„æƒé‡ï¼Œç”¨äºæƒ©ç½šå°‘æ•°è¿‡å¤§çš„æƒé‡ç»´åº¦ï¼Œä½¿åˆ†é…æ›´å¹³å‡

- è®­ç»ƒæ–¹æ³•ï¼š**å²­å›å½’ (Ridge Regression)**
$$ \min_{w, b} \frac{1}{n} \sum_{i \in [n]} (w^T x_i + b - y_i)^2 + \lambda \|w\|^2 $$
$$ \frac{\partial J(w, b)}{\partial w} = \frac{2}{n} \sum_{i \in [n]} (w^T x_i + b - y_i)x_i + 2\lambda w \in \mathbb{R}^d $$

2. è¾“å…¥æ•°æ®ä¸­å­˜åœ¨å¤§é‡æ²¡ç”¨çš„ç‰¹å¾ç»´åº¦ï¼Œä½†ä»ç„¶èµ‹äºˆäº†å®ƒä»¬éé›¶çš„æƒé‡

- é‡‡ç”¨ **L1 æ­£åˆ™åŒ–** 
$$R(f) = \|w\|_1 = \sum_{j \in [d]} |w_j|$$
- ä½œç”¨ï¼šé¼“åŠ±ç¨€ç–çš„ $w$ï¼Œå³ $w$ ä¸­å¤§éƒ¨åˆ†ç»´åº¦ä¸ºé›¶ï¼Œä»…æœ‰å°‘æ•°ç»´åº¦éé›¶

- è®­ç»ƒæ–¹æ³•ï¼š**Lassoå›å½’**
$$ \min_{w, b} \frac{1}{n} \sum_{i \in [n]} (w^T x_i + b - y_i)^2 + \lambda \|w\|_1 $$

![[AIimg/img2/image-3.png|442x323]]


é€»è¾‘å›å½’ (å®Œæ•´å½¢å¼)
- äº¤å‰ç†µæŸå¤± + L2 æ­£åˆ™åŒ–
$$
\min_{w, b} \frac{1}{n} \sum_{i \in [n]} \log(1 + e^{-y_i(w^T x_i + b)}) + \lambda \|w\|^2
$$
- æ¢¯åº¦
$$
\frac{\partial J(w, b)}{\partial w} = -\frac{1}{n} \sum_{i \in [n]} [1 - p(y_i | x_i; w, b)] y_i x_i + 2\lambda w \quad \in \mathbb{R}^d
$$




