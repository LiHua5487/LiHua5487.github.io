---
date: 2025-03-31
tags:
  - AI
---
# 决策树

结构
- 根节点包含样本全集 
- 每个非叶节点对应于一个（特征）属性测试，将样本进行划分
- 每个叶节点对应于决策结果（取多数类）

![[AIimg/img3/image.png]]

对于这个决策树，每一层划分后，样本的标签的**纯度**更高
理想情况下，每个叶节点只包含一种标签

但对于另一些决策树，纯度并没有显著提升
这说明这个属性可能和结果没有直接关系

![[AIimg/img3/image-1.png|425x268]]

我们想找出最优划分属性，划分后叶节点纯度尽可能高

在构建每一层决策树时，整个过程可以总结为
1. 遍历所有可能划分的属性，根据纯度判断划分效果
2. 选择最优的划分属性，作为这一层的划分标准
3. 用剩下的数据集与划分属性构建子树
递归进行这一过程，直到最终纯度达到要求

注意点
- 决策树是基于贪心算法的构建过程：每一层通过局部最优划分，以尝试达到全局最优
- 如果不做任何限制，决策树可能会过度细化（过拟合），通常会设置停止条件或执行剪枝操作（后剪枝或预剪枝）来限制树的复杂度，对噪声和训练数据不足问题进行处理

## 衡量纯度

### 信息增益

对于离散型随机变量 $X$，服从概率分布 $p(x)$，其**信息熵**定义为：
$$
H(X) = -\sum_x p(x) \log_2 p(x)
$$
- 如何衡量一个事件的信息量？
	对于某一事件 $x$，其发生的概率 $p(x)$ 越小，则包含的“信息量”越大
	当 $X = x$ 为确定事件，即 $p(x) = 1$ 时，其所包含的“信息量”最小（=0）
	因此可以用 $\log_2 \frac{1}{p(x)}$ 来描述该事件所包含的“信息量”
- 信息熵 $H(X)$ 为系统内不同事件（$x$ 取不同值）的“信息量”的期望值：
$$
\mathbb{E}_X \left[ \log_2 \frac{1}{p(x)} \right] = \sum_x p(x) \log_2 \frac{1}{p(x)} = -\sum_x p(x) \log_2 p(x)
$$
- 当所有事件等概率时，信息熵达到最大值 $log_2n$ ，即不确定性最大

**联合熵**：同时考虑 X 和 Y 的情况下的信息熵
$$
H(X,Y) = -\sum_x\sum_y p(x,y) \log_2 p(x,y)
$$

**条件熵**：在考虑 X 的条件下，Y 的信息熵
$$
\begin{align}
H(Y|X) &= \sum_x p(x)H(Y|X = x) \\
&= \sum_xp(x)\sum_yp(y|x) \log_2 p(y|x) \\
&= \sum_x\sum_yp(x)p(y|x) \log_2 p(y|x) \\
&= -\sum_x\sum_y p(x,y) \log_2 p(y|x)
\end{align}
$$

可以用信息熵度量样本集合 $D$ 的标签纯度
$$
H(D) = -\sum_{k \in [K]} \frac{|D_k|}{|D|} \log \frac{|D_k|}{|D|}
$$
- $| \cdot |$ 表示集合元素数目
- $D_k$ 表示集合 $D$ 中标签为 $k$ 的子集，$[K]$ 为标签集（共 $K$ 分类）
- $H(D)$ 即为经验分布下标签的信息熵
	$H(D)$ 越低，样本集合 $D$ 的纯度越高
	当 $D$ 中样本均属于某一类时，或属性只有一种取值，$H(D) = 0$

属性A对集合D的信息增益：
$$
\begin{align}
g(D, A) &= H(D) - H(D|A) \\
where\ H(D|A) &= \sum_{i \in [m]} \frac{|D^{A=a_i}|}{|D|} H(D^{A=a_i})
\end{align}
$$
- 属性 A 的可能取值为$\{a_i \,|\, i \in [m]\}$
- $D^{A=a_i}$ 表示集合 D 中样本属性 A 取 $a_i$ 的子集（即划分后的叶节点的样本）
- $H(D|A)$ 表示按照 A 划分后，样本仍具有的不确定性
- 一般来讲，信息增益越大，则使用该属性进行划分所得到的“纯度提升”越大


![[AIimg/img3/image-2.png]]

但是有一些问题，比如，如果将序号也作为一个划分属性

![[AIimg/img3/image-3.png]]

这就需要另一种标准，可以用增益率解决

### 增益率

属性 $A$ 的增益率定义为
$$
\begin{align}
g_R(D, A) &= \frac{g(D, A)}{H_A(D)} \\
where\ H_A(D) &= -\sum_{i \in [m]} \frac{|D^{A = a_i}|}{|D|} \log \frac{|D^{A = a_i}|}{|D|}
\end{align}
$$
- $H_A(D)$ 描述属性 $A$ 本身的熵（把 A 的取值当成一种标签）
- $H_A(D)$ 不同于 $H(D^{A = a_i})$
	$H(D^{A = a_i})$ 衡量某个属性划分后的子数据集是否纯净
	$H_A(D)$ 衡量属性 A 将数据划分成了多么复杂的结构（划分过程的代价）
- 当属性 $A$ 具备较多的取值时，其本身在 $D$ 上的不确定性较高，$H_A(D)$ 较大，使得对应增益率较低，一定程度上缓解了属性本身带来的高信息增益

在候选属性集合 $F$ 中选取使得划分后增益率最高的属性，即
$$
A_* = \arg\max_{A \in F} g_R(D, A)
$$

![[AIimg/img3/image-4.png]]

### 基尼指数

基尼指数 (Gini index)：另一种度量样本集合 $D$ 纯度的方法
$$
Gini(D) = \sum_{k \in [K]} \frac{|D_k|}{|D|} \left( 1 - \frac{|D_k|}{|D|} \right) = 1 - \sum_{k \in [K]} \left( \frac{|D_k|}{|D|} \right)^2
$$
- $|\cdot|$ 表示集合元素数目
- $D_k$ 表示集合 $D$ 中标签为 $k$ 的子集，$[K]$ 为标签集（$K$ 分类）
$$
Gini(D) \approx \sum_{k \in [K]} P(y = k) (1 - P(y = k))
$$
- 反映随机抽取 2 个样本，其类别不一样的概率

类似于信息熵，当 X 只有2种可能取值时，基尼指数和信息熵随取值概率的变化变化曲线如图

![[AIimg/img3/image-5.png|279x193]]

属性 $A$ 对集合 $D$ 的基尼指数
$$
Gini(D, A) = \sum_{i \in [m]} \frac{|D^{A = a_i}|}{|D|} Gini(D^{A = a_i})
$$
- 属性 $A$ 的可能取值为 $\{a_i \mid i \in [m]\}$
- $D^{A = a_i}$ 表示集合 $D$ 中样本属性 $A$ 取 $a_i$ 的子集。

在候选属性集合 $F$ 中选取使得划分后基尼指数最小的属性（平均纯度最大）
$$
A_* = \arg\min_{A \in F} Gini(D, A)
$$

![[AIimg/img3/image-6.png]]


## 离散与连续

### 连续的属性

在上面的例子里，如果将 A 从是否认真工作（离散）改为工作时长（连续）

可以将连续的属性离散化，即可以取一个划分点 $t$ ，将其分为若干类
（比如工作时长达到多少判断为认真工作，否则就不认真工作）

**二分法**

- 假设连续属性 $A$ 在训练样本集合 $D$ 上有 $m$ 个取值，升序排列为 $\{a^1, a^2, ..., a^m\}$  
- 可建立 $m-1$ 个候选的划分点，集合为  
$$
T_A = \left\{\frac{a^i + a^{i+1}}{2} \mid i = 1, ..., m-1 \right\}
$$
- 选取一个划分点，将属性 $A$ 二值化  
- 对于每个候选属性，选择最大化信息增益的候选划分点作为实际划分点，按照这个划分点，再选取信息增益最大的候选属性  
$$
A_*, t_* = \arg\max_{A \in F, t \in T_A} g(D, A, t)
$$
$$
g(D, A, t) = H(D) - \left( \frac{|D^{A, t, +}|}{|D|} H(D^{A, t, +}) + \frac{|D^{A, t, -}|}{|D|} H(D^{A, t, -}) \right)
$$
- 其中，$D^{A, t, +}$ 表示集合 $D$ 根据属性 $A$ 按 $t$ 二分后取正的子集  

![[AIimg/img3/image-7.png]]

### 连续的标签

把上面例子里的标签 是否为好科学家（离散）换为 评分（连续）
这时决策树就变为回归树，不能用信息熵等衡量好坏（它们只能衡量离散的）
可以用 L2 Loss 衡量纯度

回归树
- 保留决策树的结构  
- 通过 $L2\ Loss$ 重新定义集合 $D$ 的纯度  

纯度
- 离散：信息熵、基尼指数  
- 连续：  
$$
\text{样本均值} \quad \bar{y}_D = \frac{1}{|D|} \sum_{j \in D} y_j
$$
$$
\text{样本方差（未除以 } |D| \text{）} \quad L(D) = \sum_{j \in D} (y_j - \bar{y}_D)^2
$$
$$
L(D, A) = \sum_{i \in [m]} L(D^{A = a_i})
$$
$$
A_* = \arg\min_{A \in F} L(D, A)
$$
- 其中，属性 $A$ 的可能取值为 $\{a_i \mid i \in [m]\}$
- $D^{A = a_i}$ 表示集合 $D$ 中样本属性 $A$ 取 $a_i$ 的子集
- $\bar{y}_D$ 作为某节点标签的平均值，也表示该节点的预测标签

![[AIimg/img3/image-8.png]]


# 随机森林

集成学习= 多个个体学习器+ 结合策略 
- 个体学习器 
	- 例子：线性模型，决策树 
	- 要求：“好而不同”
- 例子：随机森林

随机森林
- 个体学习器：决策树 
- 结合策略：样本扰动+属性扰动 
- 步骤
	- 样本扰动：对于每一个决策树，对训练集进行随机采样得到一个独立的训练集
	- 属性扰动：对于每一个决策树，只选择数据集中的一部分样本特征进行子树划分训练
	- 最后，用所有训练好的决策树的平均预测（如多数类）作为对测试样本的输出

# 补充部分

## 特征提取算子

**HOG**
![[AI引论/AIimg/img3/image-9.png]]

![[AI引论/AIimg/img3/image-10.png]]
![[AI引论/AIimg/img3/image-11.png]]

![[AI引论/AIimg/img3/image-12.png]]

HOG 
- 具有几何和光学形变的良好不变性，可忽略肢体动作，适合行人检测
- 缺点是计算量大，无法处理遮挡
Haar 
- 使用模板匹配，计算速度快，适用于人脸检测
- 对旋转适应性不佳
SIFT 
- 信息量丰富，检测匹配速度快，可以部分解决遮挡、角度变化的问题
- 对边缘光滑的目标无法准确提取特征

## 支持向量机 SVM

- 对于线性可分的问题，SVM 可以直接在原空间中找到分割超平面
- 对于线性不可分的问题，SVM 会利用 **核函数 Kernel Function** 将原始特征空间映射到更高维的特征空间，在高维空间中找到一个线性可分的超平面

![[AI引论/AIimg/img3/image-13.png]]

![[AI引论/AIimg/img3/image-14.png|425x268]]

## 基于对应点的三维重建

特征提取和匹配主要有三大工序
- 检测关键点
- 生成关键点描述子，代表关键点处的局部特征
- 关键点匹配：通过两幅图像中关键点特征的两两比较，找出相互匹配的若干特征点对，然后将其作为对应点

![[AI引论/AIimg/img3/image-15.png]]

SIFT特征的优点 
- 对旋转、缩放、亮度变化保持不变性；对视角变化、仿射变换、噪声也保持一定程度的稳定性
- 独特性 Distinctiveness 好，信息量丰富
- 多量性，即使少数的几个物体也可以产生大量SIFT特征向量
- 经过优化的SIFT算法可满足一定的速度需求
- 可扩展性，可以很方便的与其他形式的特征向量进行联合



