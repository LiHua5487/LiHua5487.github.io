---
date: 2025-05-12
tags:
  - AI
---
# 知识图谱

对于知识工程，一开始采用专家系统，但只在规则明确、边界清晰、应用封闭表现较好，而且人为因素较大

知识图谱利用图结构建模知识关系，本质上是一种大规模语义网络
- 实体 entity ：表示客观存在的事 物或抽象概念
- 关系 relationships ：表示两个实体（分为主实体和客实体）之间的某种关联

![[AI引论/AIimg/img5/image.png|400x253]]

通常情况下，知识图谱以三元组的形式保存 <实体，关系，实体>

构造知识图谱的关键步骤：知识抽取、知识表示、知识推理

## 知识抽取

### 实体抽取

通常包含人名、组织/机构名、地理位置、时间/日期等标签

![[AI引论/AIimg/img5/image-1.png]]

可以使用基于人工设计的模板和规则匹配实体词，比如出现在那些词前面或者后面的词就是某种实体，但缺点很明显

基于序列标注的机器学习方法

![[AI引论/AIimg/img5/image-2.png]]

### 关系抽取

从文本中抽取出两个或者多个 实体之间的语义关系
两个实体之间可能有多种关系

![[AI引论/AIimg/img5/image-3.png]]
实现方法
- 基于模板  
	- 基于触发词/依存句法匹配  
- 基于监督学习  
	- 特征函数 + 最大熵模型  
	- 基于递归神经网络/卷积神经网络/图神经网络/双向长时记忆网络  
- 基于预训练语言模型  
	- 基于 Bert 语料库  

![[AI引论/AIimg/img5/image-4.png]]

![[AI引论/AIimg/img5/image-5.png]]

### 事件抽取

从无结构文本中自动抽取结构化事件知识（即事件实例）
包含事件发现和分类，事件要素提取（涉及的人/组织/时间/地点等）两个过程

![[AI引论/AIimg/img5/image-6.png]]

## 知识表示

### 逻辑表示法

一阶谓词逻辑（经典逻辑）

- 命题 ：具有真假意义的陈述句
	- 简单命题/原子命题：简单的陈述句，不能分解成更简单的句子，用大写字母表示（命题符号化）
	- 对于简单命题来说，如果它的真值是确定的，可被称为命题常项或命题常元，否则称为命题变项或命题变元
- 逻辑联结词：用于将多个原子命题组合成复合命题
	- ¬ 否定联结词：非
	- ∨ 析取联结词：或
	- ∧ 合取联结词：与
	- → 蕴含/单条件：$P → Q$ 表示“若 $P$ 则 $Q$” （ $P$ 为前件，$Q$ 为后件）
	- ↔ 等价/双条件：$P ↔ Q$ 表示“$P$ 当且仅当 $Q$”

| $P$ | $Q$ | ¬$P$ | $P ∧ Q$ | $P ∨ Q$ | $P → Q$ | $P ↔ Q$ |
| :-: | :-: | :--: | :-----: | :-----: | :-----: | :-----: |
|  T  |  T  |  F   |    T    |    T    |    T    |    T    |
|  T  |  F  |  F   |    F    |    T    |    F    |    F    |
|  F  |  T  |  T   |    F    |    T    |    T    |    F    |
|  F  |  F  |  T   |    F    |    F    |    T    |    T    |
- 单条件：条件为真，命题与结果真假相同；条件为假，命题始终为真
- 双条件：两个原子命题真假相同，则命题为真，否则为假

- 个体词：领域内可以独立存在的具体或抽象的客观物体（表示实体）
	- 在谓词逻辑中，个体是常量也可以是变量
	- 个体常量：具体的或者特定的个体
	- 个体变量：抽象的或者泛指的个体
	- 个体域（论域）：个体变量的值域（可以是有限的也可以是无限的）
- 谓词：刻画个体性质以及个体之间相互关系的词（表示关系）
	- 如：北京大学在北方 `North(PKU)`
	- n元谓词：含有 n 个个体符号的谓词 $P(x_1,x_2,...,x_n)$，表示 $x_1,x_2,...,x_n$ 具有关系 $P$
	- 如：北京大学在中国的北部 `North(PKU,China)`
- 函数（函数词）：从若干个体到某个个体的映射
	- 用于描述新的实体
	- 如：北京大学校长 `Principal(PKU)`

谓词与函数的区别
- 谓词描述属性和关系，可判断真假 
- 函数用已知实体描述新的实体，没有真值的概念
- 在谓词逻辑中，函数本身不能单独使用，它必须嵌入到谓词中

- 量词：用来对个体的数量进行约束
	- 全称量词 ∀：对所有的个体，都...
	    - `∀x P(x)` 为真 $⇔$ 对个体域内所有个体 $x$ 都有 $P(x)$ 为真  
	    - 如：今天下雨 `Rain(Today)`  ，每天都下雨 `∀x Rain(x)`
	- 存在量词 ∃ ：存在某个个体，使得...
	    - `∃x P(x)` 为真 $⇔$ 个体域内至少存在一个个体 $x$，使得 $P(x)$ 为真  
		- 如：张三缺席 `Absence(Zhang3)`，有人缺席 `∃x Absence(x)` 

用谓词逻辑表示知识的一般步骤
- 定义谓词及个体，确定其具体含义
- 根据所要表达的事物或概念，为每个谓词中的变量赋予特定的值
- 根据所要表达的知识的语义，用适当的逻辑联结词将各个谓词连接起来

![[AI引论/AIimg/img5/image-7.png]]

- 对于任意一个 Month，如果是5月，就判断北大是否举办了校庆；否则为真
- 整个命题的正确性取决于 `May(Month)→Hold(PKU,Anniversary)`
- 加上 `(∀Month)` 是为了明确 Month 的范围，指明该公式对所有月份都适用

表示方式不一定唯一，比如还可以把“举办校庆”当成一个一元谓词

一阶谓词逻辑存在一些缺点
- 表示能力差：只能表示确定性知识， 不能表示非确定性/过程性/启发式知识
- 管理困难：缺乏知识的组织原则， 知识库管理困难
- 效率低：把推理演算与知识含义分开，往往使推理过程冗长，系统效率降低
- 仿照人的思维逻辑设计的表示方式， 对于机器可能不是最优的

c除了逻辑表示法，还有产生式表示法、框架表示法，它们都是使用人工设置的离散符号来表示本体、关系等知识，因而称为符号表示法
在网络数据大爆炸的时代，这种人工设置的方式难以存储管理海量的知识
知识的向量化表示具有更丰富的表达空间，并且能够依托神经网络框架实现自动更新，逐渐应用在大规模知识图谱中

![[AI引论/AIimg/img5/image-8.png]]

### 向量表示法

在自然语言模型中，属性差别相近的词在词向量空间呈现平移不变性，比如King和Queen之间的向量差与Man和Woman之间的向量差接近

受到该现象的启发，考虑基于关系向量不变性约束的方法：具有相同关系的实体，在实体空间的向量差应相同，该向量差即为关系的向量化表达

## 知识推理

知识图谱推理将机器推理 Machine Reasoning 简化为事实预测或关系推理
许多现实生活中的问题（链接预测、因果推理、基于知识图谱的问答推荐）都可以表述为知识图谱推理

主要围绕关系推理，即基于图谱中已有的事实或关系推断出未知的事实或关系
一般着重考察实体、关系、图谱结构三个方面的特征信息

推理规则：逻辑等价式

1. $P → Q$ 等价于 $¬P ∨ Q$  
2. $P ↔ Q$ 等价于 $(P ∧ Q) ∨ (¬P ∧ ¬Q)$  
3. $¬(P ∨ Q)$ 等价于 $¬P ∧ ¬Q$  
4. $¬(P ∧ Q)$ 等价于 $¬P ∨ ¬Q$  
5. $R ∧ (P ∨ Q)$ 等价于 $(R ∨ P) ∧ (R ∨ Q)$  
6. $R ∨ (P ∧ Q)$ 等价于 $(R ∧ P) ∨ (R ∧ Q)$  

一阶谓词逻辑的推理过程
- 符号化过程：将自然语言中的陈述语句利用谓词公式表示
- 公式变形：利用逻辑等价式将谓词公式进行变换 
- 推理过程：利用逻辑蕴含式推出结论

形式演绎推理

- 自然演绎推理：从一组已知为真的事实出发，直接运用经典逻辑中的推理规则推出结论（类似于三段论）

---
如，设有前提：  
1. 北京大学在且仅在青年节举办校庆  
2. 北京大学今天举办校庆
问：今天是青年节吗？  
---
解：假设以下表示
- `Youth(t)`： 日期 t 是青年节  
- `Anniversary(X, t)` ： $X$ 于 $t$ 举办校庆 
- pku 表示北京大学， a 为今天的日期

则上面的两个命题可用谓词公式表示为： 
1. `∀t(Youth(t) ↔ Anniversary(pku, t))`
2. `Anniversary(pku, a)`
故 `Youth(a)` 为真，即今天是青年节  
---

- 归结演绎推理：将问题转化成多个子句，然后通过相互消解进行简化，最终推理出目标形式（类似于反证法）

---
如，现有A B C名学生选择燕园和昌平校区宿舍，规则和协商如下：
1. 由于燕园宿舍紧张，至少一名同学入住昌平校区
2. 如果𝐴选择昌平校区而𝐵选择燕园校区，则𝐶选择昌平校区
3. 如果𝐵选择昌平校区，则𝐶也选择昌平校区 
求证：𝐶一定选择昌平校区
---
第一步：将问题用谓词公式表示
前提：  
1. $A ∨ B ∨ C$  
2. $(A ∧ ¬B) → C$  
3. $B → C$  
结论：$C$
---
第二步：将谓词公式转化为CNF子句集，并将结论的否定化也加入子句集
1. $A ∨ B ∨ C$  
2. $(A ∧ ¬B) → C$  
    $⇔ ¬(A ∧ ¬B) ∨ C$  
    $⇔ ¬A ∨ B ∨ C$  
3. $B → C$  
    $⇔ ¬B ∨ C$  
4. $¬C$  
---
第三步：证明子句集是不可满足的
1. $A ∨ B ∨ C$  
2. $¬A ∨ B ∨ C$  
3. $¬B ∨ C$  
4. $¬C$  
5. $B ∨ C$    由 (1), (2)  
6. $C$           由 (3), (5)  
7. 矛盾      由 (4), (6)  
所以同学 $C$ 一定入住昌平校区  
---

# 图神经网络 GNN

图数据的特定
- 大小不确定，拓扑结构复杂（不像图片/文本在空间上是结构化的）
- 没有固定的节点顺序或者参考点 
- 目标：学习节点的表示，或全图的表示

![[AI引论/AIimg/img5/image-9.png|445x136]]

对于一个图 $G$:  
- $V$ 是 节点的集合
- $A ∈ \{0, 1\}^{|V| \times |V|}$ 是邻接矩阵（$A_{ij} = 1$ 表示节点 $i, j$ 存在边）  
- $X ∈ \mathbb{R}^{|V| \times d}$ 为节点的特征矩阵 
- $v$ 是 $V$ 中的一个节点；$N(v)$ 是 $v$ 的邻居节点的集合；$d$ 为特征的维度  

节点特征可以是:  
- 在社交网络中：用户特征，用户头像  
- 生物网络：基因表达特征，基因功能信息  
- 知识图谱：节点所代表的实体的文本 embedding  
- 当没有节点特征时：可以使用常数或度代替  

最简单的方法是将邻接矩阵和节点特征连接到一起，输入到FC网络

![[AI引论/AIimg/img5/image-10.png]]

但这么做无法处理节点数不同的图
而且，图关于节点的排列顺序是不变的，但这么做对顺序很敏感

参考 CNN 的方式，通过聚合邻居节点的特征学习中心节点表示

![[AI引论/AIimg/img5/image-11.png|644x285]]

GNN的核心任务就是设计不同的消息聚合方式

一种简单的实现方式，将邻接矩阵与特征矩阵直接相乘 $X'=AX$

![[AI引论/AIimg/img5/image-12.png]]

或者可以设置参数矩阵 W ，令 $X'=(A+I)XW$

和CNN一样，GNN也可以堆叠很多层，获得更大的感受野
第一层先聚合邻居节点的原始特征，第二层使用第一层得到的新节点作为特征再次做消息聚合
k层图神经网络即可学习到k-hop信息（从目标节点出发，在图结构中通过最多 k 跳 hop 可以到达的所有节点传递来的信息）

![[AI引论/AIimg/img5/image-13.png|463x275]]

那具体怎么计算邻居节点到中间节点的消息？

GNN 中节点表征的更新有两步
- 计算消息 Message：每个节点基于其上一层表征生成一条消息
- 消息聚合 Aggregation：目标节点从邻居节点接收这些消息，再结合自己的信息，利用聚合函数更新自身的表征

消息的计算
每个节点都会产生一条消息，将其传递给邻居节点
这些消息由节点自身的特征表示（上一层的表示）通过消息函数生成
$$m_u^{(l)} = MSG^{(l)}(h_u^{(l-1)})$$
- $m_u^{(l)}$ 是第 $l$ 层计算出的节点 u 的消息，传给邻居节点去完成聚合
- $h_u^{(l-1)}$ 是节点 u 在第 $l-1$ 层的特征表示。
- $MSG^{(l)}$ 是消息生成函数（可以是线性变换、神经网络等）

最常见消息生成的方式是线性变换
$$m_u^{(l)} = W^{(l)} h_u^{(l-1)}$$

消息的聚合
通过聚合邻居的消息，目标节点可以融合来自邻域的信息
$$h_v^{(l)} = AGG^{(l)}\left(\{m_u^{(l)} \mid u \in N(v)\}\right)$$
- $h_v^{(l)}$ 是目标节点 v 在第 $l$ 层更新后的表示。
- $N(v)$ 是节点 v 的邻居节点集合
- $AGG^{(l)}$ 是聚合函数（如求和，均值，取max）

除了聚合邻居的消息，目标节点还需要结合自己的信息
$$m_v^{(l)} = B^{(l)} h_v^{(l-1)}$$
- $B^{(l)}$ 是针对节点自身特征的更新权重

最后将自身消息与聚合的邻居消息组合，方式包括拼接 concatenate 或求和
$$h_v^{(l)} = concat\left(AGG^{(l)} \left(\{m_u^{(l)} \mid u \in N(v)\}\right), m_v^{(l)}\right)$$

容易得到，以上方法对于节点排列是不变的，而且对于不同的节点个数都能用

GNN通过多层消息传递层学习节点表示，最后可以通过一个全局 pooling 层学习到全图表示

![[AI引论/AIimg/img5/image-14.png]]






