
# 基本概念

检索：在记录集合中找到 “关键码值=给定值” 的记录，或找到关键码值 “符合特定约束条件” 的记录集

>如果记录是一个数字，那关键码就是它本身；我们实际上是把 {关键码，值} 这个整体作为一个元素进行存储，要找关键码为 K 的记录对应的值

检索的主要操作：关键码的比较
**平均检索长度** (Average Search Length, ASL) ：检索过程中对关键码的平均比较次数，衡量检索算法优劣的时间标准

$$
ASL = \sum_{i=1}^{n} P_i C_i
$$

- $P_i$ ：为检索第 $i$ 个元素的概率
- $C_i$ ：找到第 $i$ 个元素所需的比较次数

>假设线性表为 $(a, b, c)$，检索 $a,b,c$ 的概率分别为 0.4, 0.1, 0.5，则顺序检索算法的平均检索长度为 $0.4 \times 1 + 0.1 \times 2 + 0.5 \times 3 = 2.1$ ，即平均需要 2.1 次给定值与表中关键码值的比较才能找到待查元素

提高检索效率的方法
- 预排序：在检索之前，先把记录按某种方式进行排序
- 建立索引：为每个记录搞一个索引，需要耗费额外的空间来存储这些索引
- 散列技术：搞一个哈希表

# 线性表的检索

## 顺序检索

- 过程：对线性表里的记录，逐个将其关键码与给定值（要查找的值）比较
- 物理存储：可以顺序、或者链接
- 排序要求：无

下面是一个采用了监视哨的顺序检索代码

```cpp
// 查找关键字K是否在序列当中
template <class Type> int SeqSearch(vector<Item<Type>*>& dataList, int n, Type k)
{
    int i = n;
    dataList[0] = k; // 监视哨，将第0个元素设为待检索值
    while(dataList[i] != k)
        i--;
    return i; // 返回元素位置，如果是0说明检索失败
}
```

假设搜索每个关键码是等概率的，即 $P_i = 1/n$ ，则搜索成功时的 ASL 为

$$
\begin{align}
ASL_S &= \sum_{i=1}^{n} P_i*(n-i+1) = \frac{1}{n} * \sum_{i=1}^{n} (n-i+1) \\
&= \frac{n+1}{2}
\end{align}
$$

搜索失败时，由于设置了一个监视哨，所以需要 $n+1$ 次比较，即

$$
ASL_F = n+1
$$

假设搜索成功的概率为 $p$ ，搜索失败的概率为 $q = (1-p)$ ，则整体的 ASL 为

$$
\begin{align}
ASL &= p \cdot ASL_S + q \cdot ASL_F \\
&= p \cdot \frac{n+1}{2} + q \cdot (n+1) \\
&= p \cdot \frac{n+1}{2} + (1-p)(n+1) \\
&= (n+1)(1-p/2)
\end{align}
$$

可得 $(n+1)/2 < ASL < (n+1)$ 

- 优点：插入元素可以直接添加到表尾，这是 $\Theta(1)$ 的
- 缺点：检索时间为 $\Theta(n)$ ，太慢了

## 二分检索

- 排序要求：序列有序（以从小到大为例）
- 过程：将序列的中间元素与给定值比较，如果给定值更大，说明应该在右半段序列找；如果给定值更小，说明应该在左半段找；如果一样就检索成功

![[数算/img/img10/image.png]]

```cpp
// 为与顺序检索保持一致，位置0不存放实际元素
template <class Type> int BinSearch (vector<Item<Type>*>& dataList, int length, Type k){
    int low=1, high=length, mid;
    while (low<=high) {
        mid = (low+high)/2;
        if (k < dataList[mid]->getKey())
            high = mid-1;
        else if (k > dataList[mid]->getKey())
            low = mid+1;
        else return mid; // 检索成功
    }
    return 0; //检索失败
}
```

- 最大检索长度为完全二叉树的高度，即为 $\lceil \log_2 (n+1) \rceil$ 
- 失败时的检索长度是 $\lceil \log_2 (n+1) \rceil$ 或 $\lfloor \log_2 (n+1) \rfloor$ 
- 成功时的平均检索长度为

$$
\begin{align}
ASL &= \frac{1}{n} \left( \sum_{i=1}^{j} i \cdot 2^{i-1} \right) \\
&= \frac{n+1}{n} \log_2 (n+1) - 1 \\
&\approx \log_2 (n+1) - 1
\end{align}
$$

- 优点：平均与最大检索长度相近
- 缺点：需要预排序，必须顺序存储（因为要访问中间元素），不易更新（插入元素时需要重新调整以保证仍然有序，删除元素后需要移动其它元素填补删除位置）

## 分块检索

是顺序检索与二分检索的折中，既能较快的检索，又能较灵活的更改
- 把序列分为若干块（一些块可能不满），块内无序，不同块之间是有序的（前块中最大值 < 后块中最小值）
- 构建一个索引表，记录各块中的最大关键码、各块起始位置、块中有效元素个数，由于块间是有序的，则索引表是分块有序的
- 检索时，先与各块最大关键码比较，确定在哪一块，再在块内顺序检索

![[数算/img/img10/image-1.png]]

若在索引表中用顺序检索，设 $s$ 为块内元素个数，则块间和块内 ASL 分别为

$$
ASL_b = \frac{b+1}{2} \quad ASL_w = \frac{s+1}{2}
$$

总共的 ASL 为

$$
\begin{align}
ASL &= \frac{b+1}{2} + \frac{s+1}{2} = \frac{b+s}{2} + 1 \\
    &= \frac{n+s^2}{2s} + 1
\end{align}
$$

当 $s = \sqrt{n}$ 时，ASL 取最小值

$$
ASL = \sqrt{n} + 1 \approx \sqrt{n}
$$

若在索引表中用二分检索，则总共的 ASL 为

$$
\begin{align}
ASL &= ASL_b + ASL_w \\
    &\approx \log_2(b+1) - 1 + \frac{s+1}{2} \\
    &\approx \log_2(1 + \frac{n}{s}) + \frac{s}{2}
\end{align}
$$

优点
- 插入、删除容易
- 无大量记录移动
缺点
- 增加一个辅助索引表
- 需要对记录进行分块排序
- 元素大量插入/删除，或分布不均匀时性能下降

# 散列表的检索

基于关键码比较的检索的时间复杂度与序列规模相关，当 $n$ 很大时耗时很长；理想情况是根据关键码值，直接找到记录的存储地址，不需把待查关键码与候选记录进行逐个比较

比如对于数组，我们能直接根据下标访问元素，散列表 / 哈希表的想法与之类似，用一个散列函数 $h$ ，将关键码 $k$ 映射到记录的存储地址 $h(k)$ 

一般的，散列表是一个一维数组，我们把 {关键码，值} 这个对存到里边，但是不一定是相邻存储的，中间会有空位，而散列函数映射到的存储地址 $h(k)$ 就是这个数组的索引值

**负载因子 / 装填因子**： 衡量散列表有多满

$$\alpha = \frac{n}{M}$$

- $n$ ：散列表中已经存了的节点数（可能一个位置存了多个节点）
- $M$ ：散列表空间大小（索引范围，即有几个位置，每个位置称为“槽”）

冲突：不同的 key 映射到同一个地址，这两个 key 就叫一对同义词，构造散列表时，我们希望找到一个函数，使得尽可能减少冲突的发生

## 散列函数

一般来讲，散列函数的输入是一个整数，如果关键码不是整数而是字符串等其它类型，需要先转换为整数

### 除余法

$$h(x)=x\%M$$

- 这里 $M$ 需要根据待检索记录中的关键码的情况进一步确定， 一般可以取散列表的长度，保证 $h(x)$ 能涵盖到散列表所有位置
- 如果 $M$ 取偶数，这意味着 $h(x)$ 与 $x$ 奇偶性相同，如果待检索记录中偶数的出现频率比较高，那映射到的地址为偶数的频率也会很高，会倾向于往偶数地址”挤“，分布不均匀，这就增加了冲突发生的可能性（奇数同理）
- 如果 $M$ 取 $10^k$ ，那 $h(x)$ 就仅仅由 $x$ 的后 $k$ 位决定，也会导致类似的问题
- 所以**通常取 $M$ 为一个质数**，这样 $h(x)$ 就倾向于依赖 $x$ 的所有位，增加了均匀分布的概率（$M$ 也可以不设为常数，而是在程序运行时确定）

### 乘余取整法

$$
\text{hash(key)} = \lfloor n \cdot (A \cdot \text{key} \% 1) \rfloor
$$

- $A$ 是一个 $(0,1)$ 内的常数，$n$ 是另一个系数
- $A \cdot \text{key} \% 1$ 表示取 $A \cdot key$ 的小数部分

优点是对于 $n$ 的选择无关紧要；而 $A$ 的选择与待检索的记录的数据特征有关，一般取黄金分割比 $A=\frac{\sqrt{5}-1}{2}$ 

### 平方取中法

先通过求关键码的平方来扩大差别，再取其中的几位或其组合作为散列地址

比如一组二进制关键码

$$(00000100, 00000110, 00000101, 000001001, 00000111)$$

平方结果为

$$(00010000, 00100100, 01100010, 01010001, 00110001)$$

若表长为 4 个二进制位，则可取中间四位作为散列地址

$$(0100, 1001, 1000, 0100, 1100)$$

实际应用中，需要根据关键码的具体情况，选取合适的散列函数，一般平方取中法最接近于随机化

### 数字分析法

设关键码是 $n$ 个 $d$ 位数（不足的前面补 0 ），每一位可能有 $r$ 种不同的符号，这 $r$ 种不同的符号在各位上出现的频率不一定相同，可根据散列表的大小，选取其中各符号均匀分布的若干位作为散列地址，为此定义指标 $\lambda_k$ 

$$
\lambda_k = \sum_{i=1}^r \left( \alpha_i^k - \frac{n}{r} \right)^2
$$

- $\alpha_i^k$ ：第 $i$ 个符号在第 $k$ 位上出现的次数
- $\frac{n}{r}$ 代表了各种符号在 $n$ 个数中均匀出现的期望值
- $\lambda_k$ 值越小，第 $k$ 位符号分布越均匀，若散列表索引取值范围有 $d$ 位，就取前 $d$ 个最小的

数字分析法仅适用于事先明确知道表中所有关键码每一位数值的分布情况，完全依赖于关键码集合，如果换一个关键码集合，选择哪几位数据要重新确定

### 基数转换法

把关键码看成是另一进制上的数后，再把它转换成原来进制上的数，取其中若干位作为散列地址；一般转换的基数要大于原来的基数，并且两个基数要互素

比如给定一个十进制数的关键码是 $(210485)_{10}$，把它看成以 13 为基数的十三进制数 $(210485)_{13}$，再把它转换为十进制

$$
(210485)_{13} = 2 \times 13^5 + 1 \times 13^4 + 4 \times 13^2 + 8 \times 13 + 5 = (771932)_{10}
$$

假设散列表长度是 10000（即下标 0~9999），则可取后 4 位 1932 作为散列地址

### 折叠法

将关键码分割成位数相同的几部分（最后一部分的位数可以不同），然后取这几部分的叠加和作为散列地址，其中叠加和有以下计算方式
- 移位叠加：把各部分的最后一位对齐相加
- 分界叠加：沿各部分的分界来回折叠，然后对齐相加
- 最后都需要舍去超出位数的部分

![[数算/img/img10/image-2.png]]

# 冲突的解决方法

## 开散列方法

把同义词连接到一个链表中，但是这样可能导致存储的记录数量超过散列表的空间大小，即负载因子 $\alpha > 1$ ，这意味着冲突发生的太多了，需要避免这种情况

### 拉链方法

适用于内存，把每个槽视为一个链表的表头，出现同义词就连到后面
- 可以根据输入顺序、访问频率、值的大小来对链表中的记录排序
- 如果按值排序（比如从小到达），检索时一旦遇到一个比待检索的关键码大的值，就可以停止检索，因为后面的肯定都比它大，不可能相同

计算 ASL 时，在链表上，如果对比失败，就往下移动，对于失败情况，可以认为最后移动到了“空节点”上，所以对比次数会额外 +1 

![[数算/img/img10/image-3.png]]

优点
- 处理冲突简单，不同基地址冲突彼此独立，平均查找长度短
- 链表结点动态申请，适合于表长不确定情况
- 拉链法中可以出现 $\alpha \geq 1$ 的情况，且结点较大时，拉链法中增加的指针域可忽略不计，对于同样的散列表空间，能存更多东西
- 用拉链法构造的散列表，删除结点易于实现

但是如果散列表元素存储在磁盘，用拉链法就不适用了，因为同义词表中的元素可能存储在不同的磁盘页块中，这就导致在检索一个特定关键词值时会引起多次磁盘访问，从而增加了检索时间

>页块：磁盘被分成固定大小的单元，称为“块”或“页”，是磁盘读写的最小单位，大小通常是固定的；这是一个逻辑层面的概念，一个页块通常对应着几个连续的扇区；一个页块里会存放着若干记录

### 桶式散列

适用于外存，一个外存文件会由很多页块组成，可以把这些页块分配到不同的桶中，各个页块用”指针“连接（不是内存地址，而是磁盘上的页块地址 / 块号），桶式散列表就是由这一堆桶组成，散列表 / 桶目录表存储每个桶的第一个页块的编号（可以存在内存，也可以存在外存）

磁盘访问性能
- 如果桶目录表存在外存，那获取起始地址需要访问一次外存
- 逐个检查桶内各页块，则平均访问外存次数为桶内页块数一半
- 对于修改、插入等其他运算也需要访问外存

## 闭散列方法

把同义词放到散列表后面的空位，通过探查函数关联这些同义词
- $d_0 = h(K)$ 称为 $K$ 的基地址
- 当冲突发生时，使用探查函数 $p(K,i)$ 为关键码 $K$ 生成一个候选的散列地址序列，称为探查序列 $d_1, \dots, d_{M-1}$ ，其中 $d_i = d_0 + p(K, i), \quad (0 < i < M)$
- 插入 $K$ 时，若基地址已被占用，则按探查序列依次查找，存到第一个空位上；如果没有空位，就报告溢出

检索时与插入类似，也是沿着探查序列找，注意这时也需要保证探查序列存在一个空位，不然就可能一直比较陷入循环（因为探查序列可能是循环的，不过也可以通过限制序列长度，达到最大长度终止）

### 线性探查

沿着散列表不断往下移动一格

$$d+1, d+2, ......, M-1, 0, 1, ......, d-1$$

但这会导致**聚集**的问题：很多不同的记录插入到散列表时，都会争夺同一后继散列地址

比如一组关键码为（26，36，41，38，44，15，68，12，06，51，25），散列表长度 $M = 15$ 
- 利用除余法构造散列函数，取小于 $M$ 的最大质数 $P = 13$，则散列函数为$h(K) = K \% 13$ 
- 插入 15 时，与 41 发生冲突，就放到下一个位置 3 号位
- 插入 68 时，与 15 发生冲突，就得放到 4 号位
- 12、51、25 与 38 发生冲突，沿着散列表往后放，依次放到 13、14、1 号位

![[数算/img/img10/image-4.png]]

此时如果还要往里插入元素，放到 7 号位的概率就是 $\frac{9}{13}$ （直接放到 7 ，或往下移动到 7 ），而放到其它位置的概率都比较小，但在理想情况下，表中的每个空槽都应该有相同的机会接收下一个要插入的记录

成功查找的 $ASL_{succ}$

$$
ASL_{succ} = \frac{1}{11} \sum_{i=1}^{11} C_i = \frac{1}{11}(1*6+2+2+2+3+5) = \frac{20}{11}
$$

失败查找的 $ASL_{unsucc}$

$$
\begin{align}
ASL_{unsucc} &= \frac{8+7+6+5+4+3+2+1+1+1+2+1+11}{13} \\
&= \frac{52}{13} = 4
\end{align}
$$

>查找失败时，以 0 为例，如果待查找值经过散列函数映射为 0 ，就需要沿着 26 → 25 ... → 6 一直往下移动，直到移动到空位，总共经过了 8 次比较（与空位的比较 / 判空也算）

---

一种改进方案是，探查时每次相距 $c$ 个槽，即

$$p(K,i)=ic$$

但这并没有完全解决聚集的问题，比如对于 $c=2$ ，$k=3$ 和 $k=5$ 的探查序列仍会纠缠在一起

### 二次探查

探查增量序列依次为：$1^2, -1^2, 2^2, -2^2, \dots$，即探查函数为

$$
p(K, 2i-1) = i * i,\quad p(K, 2i) = -i * i
$$

探查序列为

$$
d_{2i-1} = (d + i^2) \% M,\quad d_{2i} = (d - i^2) \% M
$$

例：使用一个大小 $M = 13$ 的表，对于关键词 $k_1$ 和 $k_2$，$h(k_1) = 3$，$h(k_2) = 2$ 
- $k_1$ 的探查序列是 $3, 4, 2, 7, \dots$ 
- $k_2$ 的探查序列是 $2, 3, 1, 6, \dots$ 
尽管 $k_2$ 会把 $k_1$ 的基位置作为第 2 个选择来探查，但是这两个关键词的探查序列此后就分开了

### 伪随机数序列探查

探查函数是一个随机序列，即

$$
p(K, i) = \text{perm}[i - 1]
$$

- $\text{perm}$ 是一个长度为 $M - 1$ 的数组， 是 1~M-1 的随机序列

---

前面的”聚集“问题可以进一步分为两种
- 基本聚集
	- 当多个关键码映射到邻近的槽位时，它们的探查序列会重叠并合并成一个连续的较长区块
	- 主要出现在线性探查中
- 二级聚集
	- 两个具有相同起始哈希值的键会遵循完全相同的探查序列
	- 与基本聚集的区别：基本聚集是不同起始哈希值的键合并成大区块；二级聚集是相同起始哈希值的键形成的链状竞争（不同哈希值的键则一般不会相互竞争）
	- 二次探查和伪随机探查可以消除基本聚集，但无法避免二级聚集，因为它们的探查增量序列和关键码 $K$ 无关

### 双散列探查

在原有散列函数 $h_1(x)$ 的基础上，引入另外一个散列函数 $h_2(x)$ 用于计算探查序列，采用线性探查的方式

$$p(K,i)=i\cdot h_2(K),\quad d_i=(d+i\cdot h_2(k))\%M$$

- $h_2(k)$ 需要与 $M$ 互素，以保证均匀分布
- 不易产生聚集，但计算量增大

# 散列表的算法实现

## 插入

将一个键值对 $e$ 插入散列表中，需要保证不存在重复的键值对

```cpp
bool HashInsert(const Elem& e) {
    int home = h(getkey(e)); // 基位置
    int i = 0;
    int pos = home; // 探查位置
    while (!eq(EMPTY, HT[pos])) { // 如果位置不为空，一直探查
	    // 插入的键值对与表中元素的重复
        if (eq(e, HT[pos])) return false;
        i++;
        pos = (home + p(getkey(e), i)) % M; // 下一探查地址
    }
    HT[pos] = e;
    return true;
}
```

## 检索

查找键 $K$ 对应的值，沿着探查序列一直比较下去，如果碰到空位则检索失败

```cpp
bool HashSearch(const Key& K, Elem& e) const {
    int home = h(K);
    int i = 0;
    int pos = home;
    while (!eq(EMPTY, HT[pos])) {
        if (eq(K, getkey(HT[pos]))) {
            e = HT[pos]; // 找到键值对，写入e
            return true;
        }
        i++;
        pos = (home + p(K, i)) % M;
    }
    return false;
}
```

## 删除

删除记录的时候，有两点需要重点考虑
- 删除记录不能影响后续检索
- 释放存储位置能为将来所用

开散列方法可以直接删除，闭散列方法中不能这么干，不然会影响检索，只能做标记，比如之前的例子，如果把 15 删了，那检索 68 时会碰到空位，检索失败

![[数算/img/img10/image-4.png]]

## 带墓碑的插入与删除

所以在闭散列中，需要一个标记，将删除的位置和空位区分开，这个标记称为**墓碑**，表示这个位置曾经有元素，现在被删了，遇到墓碑仍然往下走

但是在插入的时候，遇到墓碑，不能直接把元素放到里面，比如上面 38、12、51、25 相互冲突，沿着探查序列排布，如果删除 12 ，此时要插入 51 ，从 38 所在位置往下走，如果直接放到墓碑位置 13 号位，就和 14 号位的元素重复了

所以插入时遇到墓碑，先把这个位置记录下来，并继续往下走，走了一遍发现没有重复的元素，就放到墓碑位置

```cpp
bool HashInsert(const Elem &e) {
    int insplace; // 最终插入位置
    int home = h(getkey(e));
    int i = 0;
    int pos = home;
    bool has_tomb = false;
    while (!eq(EMPTY, HT[pos])) {
        if (eq(e, HT[pos])) return false;
        // 记录第一个墓碑的位置
        if (eq(TOMB, HT[pos]) && !has_tomb) {
            insplace = pos;
            has_tomb = true;
        }
        pos = (home + p(getkey(e), ++i)) % M;
    }
    if (!has_tomb) insplace = pos;  // 没有墓碑就用最后移到的空位
    HT[insplace] = e;
    return true;
}
```

## 散列方法的效率分析

- 衡量标准：插入、删除、检索操作所需的ASL
- 插入和删除都是基于检索进行的，插入时需要通过检索来判断有没有重复元素，并放到空位（相当于一次失败的检索）；删除时需要先检索到给定键值的元素

散列效率与负载因子 $\alpha$ 有关
- $\alpha$ 较小时，散列表比较空，插入的记录容易直接放到基地址上
- $\alpha$ 较大时，插入时很可能要靠冲突解决策略找到空位，随着 $\alpha$ 增加，越来越多的记录有可能放到离其基地址更远的地方

|       | 成功检索（删除）                                              | 不成功检索（插入）                                                 |
| ----- | ----------------------------------------------------- | --------------------------------------------------------- |
| 开散列法  | $1 + \frac{\alpha}{2}$                                | $\alpha + e^{-\alpha}$                                    |
| 双散列法  | $\frac{1}{\alpha} \ln \frac{1}{1 - \alpha}$           | $\frac{1}{1 - \alpha}$                                    |
| 线性探查法 | $\frac{1}{2} \left( 1 + \frac{1}{1 - \alpha} \right)$ | $\frac{1}{2} \left( 1 + \frac{1}{(1 - \alpha)^2} \right)$ |

![[数算/img/img10/image-5.png]]


|     | 堆积现象 | 结构开销 | 插入/删除 | 查找效率 | 估计容量 |
| --- | ---- | ---- | ----- | ---- | ---- |
| 开散列 | 无    | 有    | 效率高   | 效率高  | 不需要  |
| 闭散列 | 有    | 无    | 效率低   | 效率低  | 需要   |

散列方法的代价（平均探查次数）一般接近于访问一个记录的时间，效率高，比需要 $\log n$ 次记录访问的二分检索好得多
- 不依赖于 $n$，只依赖于负载因子 $\alpha = n / M$
- 随着 $\alpha$ 增加，预期的代价也会增加
- 经验表明，负载因子的临界值是 $0.5$ ，小于临界值时，大部分操作的分析预期代价都小于 $2$ ；大于临界值，性能就会急剧下降

散列表的插入和删除操作如果很频繁，将降低散列表的检索效率
- 大量的插入操作，将使得负载因子增加，从而增加了同义词子表的长度，也就是增加了平均检索长度
- 大量的删除操作，将增加墓碑的数量，增加记录本身到其基地址的平均长度

对于插入和删除操作比较频繁的散列表，可以定期对表进行重新散列
- 把所有记录重新插入到一个新的表中
- 清除墓碑
- 把最频繁访问的记录放到其基地址








