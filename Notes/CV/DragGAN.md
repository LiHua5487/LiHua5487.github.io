
# DragGAN

DragGAN 的核心目标是：让用户能够通过选定并拖动图片上的几个点（指定目标位置），来精准地改变图像内容，比如调整姿势、形状、表情

DragGAN 并不是直接对原图像进行编辑的，而是调整一个图像在 StyleGAN 的潜在编码 $w$ ，把调整后的向量 $w'$ 输入到 StyleGAN 的生成器得到调整后的图像，其中调整过程是迭代的进行下面步骤
- 运动监督：改变图像，使点更接近目标位置
- 点追踪：在改变后的图像上，找到选定的点移动到了哪里

## StyleGAN

传统的 GAN 是学习一个生成器，把一个随机噪声 $z$ 转换为一个图像，但是这玩意解释性很差（我们很难确定改变了 $z$ 的一些地方，得到的图像会怎么变，这是不可控的，但我们希望能根据选定点来改变图像）

StyleGAN 额外搞了个映射网络，把 $z$ 映射为一个向量 $w\in \mathbb{R}^{512}$ ，这是一个潜在编码 latent code ，而后把它复制到生成器的每一层，作为风格向量给到 AdaIN ，这个 $w$ 会具有更强的可解释性

![[CV/img/draggan/image.png]]

论文中采用的是 StyleGAN2 ，这是对 StyleGAN 的一个改进

---

论文中采用的是 $\mathcal{W}+$ 空间而非 $\mathcal{W}$ 空间
- $\mathcal{W}$ 空间：在 StyleGAN 中，$z$ 通过映射网络得到的一个向量 $w$ 构成的空间
- $\mathcal{W}+$ 空间：每个生成器层使用不同的 $w$ 向量，如果生成器有 $l$ 层，那么空间维度就是 $l\times 512$ 的
- DragGAN 主要编辑的是空间属性（位置、姿势、形状），这些主要由生成器的前几层控制，论文中只对前 6 层的 $w$ 进行调整，后面几层的不变

---

由于 StyleGAN 中图像是输出内容，我们并不能直接获取一个图像的 $w$ ，需要通过一些其它手段（GAN inversion，如 PTI）

## 运动监督

假设希望改变一个图像，我们获取了其 $w$ ，并指定了一系列调整点 $p_1,\cdots,p_n$ 和相应的目标点 $t_i$ 

考虑生成器的一个中间层输出的特征图 $F$ （论文采用 StyleGAN2 第 6 个 block 后的特征图），通过双线性插值方法变到图像的大小

这样就能直接把选定点（红）与目标点（蓝）标记到 $F$ 中，记一点 $p$ 处的特征值为 $F(p)$ 

![[CV/img/draggan/image-1.png]]

要移动选定点，就是想让调整后的 $w'$ 对应的特征图 $F'$ 中，目标位置附近的特征与原先选定点附近的特征尽可能相似

对于每个 $p_i$ ，我们选取其周围半径为 $r_1$ 的圆形区域，记为 $\Omega_1(p_i,r_1)$ ，其中的像素记为 $q_j$ ；计算 $p_i$ 到 $t_i$ 的方向向量 $d_i$ （单位向量）

$$
d_i=\frac{t_i-p_i}{\|t_i-p_i\|}
$$

我们希望这个区域朝着目标方向移动，即最小化下式

$$
\sum_{q_j \in \Omega_1(p_i, r_1)} 
\| F(q_j) - F'(q_j + d_i) \|_1
$$

>也可见 $F(q_j).detach()-F(q_j+d_i)$ 这种表示，这里 $F$ 就是调整对象，相当于上面的 $F'$ ；而 `.detach()` 表示去除梯度，也就是不进行调整的版本，即一开始的 $F$ 

由于有多个 $p_i$ ，所以总共的 loss 是

$$
\sum_{i=1}^{n} \sum_{q_j \in \Omega_1(p_i, r_1)} \|F(q_j) - F'(q_j + d_i)\|_1
$$

DragGAN 还允许用户选择图像的一个区域，只对这个区域进行改变，记为一个 mask $M$ （1 为可改变部分），则 loss 变为下面这样，其中 $F_0$ 是最初的图像的特征图（开始迭代之前，而 $F$ 是某次迭代的特征图）

$$
\sum_{i=1}^{n} \sum_{q_j \in \Omega_1(p_i, r_1)} \|F(q_j) - F'(q_j + d_i)\|_1 + \lambda \|(F' - F_0) \cdot (1 - M)\|_1
$$

在每次迭代中，只进行一个 step 的更新，得到调整后的 $w'$ 与 $F'$ ，由于 $d_i$ 是个单位向量，而且由于梯度下降方向等原因，这并不会让 $p_i$ 到达 $t_i$ （所以要多次迭代），但也不一定直接到达 $p_i+d_i$ （所以需要用点追踪的方式找到）

## 点追踪

传统的点追踪方式是利用光流，或者使用神经网络预测，但它们通常在真实世界的视频上训练，而 GAN 生成的连续图像帧之间存在一些特殊“瑕疵”，所以论文采用了一种独创的点追踪方式

由于运动监测中，每个点都不会移动的太远，所以只需要在原先位置 $p_i$ 的一个邻域内，根据特征值做最近邻匹配即可（即找到长得最像的地方）

取 $p_i$ 附近的正方形区域 $\Omega_2(p_i,r_2)=\{(x,y)\vert\  \lvert x - x_{p,i}\rvert < r_2, \lvert y - y_{p,i}\rvert < r_2\}$ ，其中的像素记为 $q_j$ ；记 $f_i=F_0(p_i)$ ，这代表了 $p_i$ 处的特征应该是什么样的，则要找的位置 $p_i'$ 就是

$$
p_i' = \arg\min_{q_j \in \Omega_2(p_i, r_2)} \|F'(q_j) - f_i\|_1
$$

>这里使用 $f_i=F_0(p_i)$ 而非 $F(p_i)$ ，是为了防止越来越歪（因为 $F$ 与 $F_0$ 相比，特征发生了变化，如果用 $F(p_i)$ 偏差会一直累积）


















