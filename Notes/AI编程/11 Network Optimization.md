
# 神经网络的混合精度训练

浮点数表示的精度有 float16 float32 等
- 精度小：计算快，但可能会产生数值溢出（主要是下溢，即数太小了；FP16最小非零值约 6e-8 ）、舍入误差（比如 $\frac{1}{3}$ 存储为 0.3）的问题
- 精度大：数值稳定，但计算较慢，而且内存占用大

![[AI编程/imgs/img11/image.png]]

在神经网络中，可以在不同部分采用不同的精度，具体包括
- 使用 FP16 进行前向传播与反向传播中梯度的计算
- 使用 FP32 存储并更新模型参数
- 通常会应用损失缩放 Loss Scale ，即将损失值放大后再计算梯度，防止小梯度被忽略

![[AI编程/imgs/img11/image-1.png]]

之所以在更新参数时使用 FP32 ，是因为更新公式中 `lr * gradient` 这一项可能非常小，所以先把梯度从 FP16 转换为 FP32 ，再更新参数；而在前向传播与梯度计算时，模型参数会从 FP32 转换为 FP16 

---

但是计算梯度时仍然是用 FP16 ，无法避免数值下溢的问题，这可以通过损失放缩解决，即反向传播前，先把 loss 放大一个倍数（使用 FP32），反传时计算的就是放大后的梯度，更新时只需要放缩回来（使用 FP32）

![[AI编程/imgs/img11/image-2.png]]





























